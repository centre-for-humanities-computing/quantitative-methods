{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc9b2642",
   "metadata": {},
   "source": [
    "# Exercise 3: Topic-modeling with unsupervised learning\n",
    "\n",
    "In this exercise we will approach **topic modeling** as an unsupervised learning problem. The first issue, though, is to have a working definition of what we mean by a *topic*.\n",
    "\n",
    "A common choice is to model topics as *groups of words that co-occur* in documents about certain topics, e.g., \"dog\", \"bone\", \"leash\" is expected to occur more frequently in documents *about dogs*. This reduces topic modeling to finding these groups of co-occuring words and models documents as collections of topics in this regard. \n",
    "\n",
    "This fundamental assumption is going to guide our approach to trying to detect these co-occurence patterns in a toy dataset of news documents. Similar to the exercises about supervised learning we are going to use our knowledge of the data and domain to make assumptions that are translated into increasingly refined *inductive biases* and *data representations*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff131976",
   "metadata": {},
   "source": [
    "## Setup: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36baa819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn for traditional ML\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import hdbscan\n",
    "\n",
    "# Dimensionality reduction\n",
    "import umap\n",
    "\n",
    "# Sentence transformers for modern embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")\n",
    "print(\"\\nNote: If you get import errors, install missing packages:\")\n",
    "print(\"  pip install sentence-transformers scikit-learn pandas matplotlib seaborn hdbscan umap-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5616d17",
   "metadata": {},
   "source": [
    "### Pre-download Embedding Model\n",
    "\n",
    "We'll download the sentence embedding model now so it's ready for Part 5. This is a small (~80MB), fast model that is trained as a language model to capture semantic meaning (similar to the one we trained in Exercise 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e2d588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-download the embedding model (takes ~30 seconds first time)\n",
    "print(\"Downloading sentence embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"✓ Model ready!\")\n",
    "print(f\"  Model: all-MiniLM-L6-v2\")\n",
    "print(f\"  Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"  Note: Larger models like 'all-mpnet-base-v2' give better quality but are slower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a6d10",
   "metadata": {},
   "source": [
    "## Preliminaries: Load and Explore the Dataset\n",
    "\n",
    "We'll use the **BBC News dataset**: 2,225 news articles from the BBC across 5 categories (business, entertainment, politics, sport, tech). This is a clean, well-structured dataset that's fairly easy to work with.\n",
    "\n",
    "**For the adventurous**: The code below works with any dataset having `text` and `category` columns. You can substitute:\n",
    "- **20 Newsgroups**: International online discussions (use `sklearn.datasets.fetch_20newsgroups`)\n",
    "- **Reuters**: International financial news (https://www.kaggle.com/datasets/nltkdata/reuters)\n",
    "- **Your own data**: Any CSV with a text column! (if you don't have categories you can just skip the parts using them)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5615c482",
   "metadata": {},
   "source": [
    "Now we're going to:\n",
    "1. Load the data\n",
    "2. Get some stats for the dataset to understand it better\n",
    "3. Show the distribution of documents across categories \n",
    "4. DIsplay a few example documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a518be1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BBC News dataset\n",
    "# Download from: https://www.kaggle.com/datasets/yufengdev/bbc-fulltext-and-category\n",
    "# Or use the provided CSV file\n",
    "\n",
    "# Option 1: Load from CSV file\n",
    "try:\n",
    "    df = pd.read_csv('../data/bbc-text.csv')\n",
    "    print(\"✓ Loaded BBC News from local file\")\n",
    "    # Basic info\n",
    "    print(f\"\\nDataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(f\"{df.head()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"BBC News CSV not found. Please download it from https://www.kaggle.com/datasets/yufengdev/bbc-fulltext-and-category and place it in the '../data/' directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c56dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Number of documents\n",
    "n_docs = len(df)\n",
    "print(f\"Total documents: {n_docs:,}\")\n",
    "\n",
    "# Category distribution\n",
    "print(f\"\\nCategories ({df['category'].nunique()}):\")\n",
    "category_counts = df['category'].value_counts()\n",
    "for cat, count in category_counts.items():\n",
    "    print(f\"  {cat}: {count} ({100*count/n_docs:.1f}%)\")\n",
    "\n",
    "# Document length statistics\n",
    "df['doc_length'] = df['text'].str.split().str.len()\n",
    "print(f\"\\nDocument length (words):\")\n",
    "print(f\"  Mean: {df['doc_length'].mean():.0f}\")\n",
    "print(f\"  Median: {df['doc_length'].median():.0f}\")\n",
    "print(f\"  Min: {df['doc_length'].min()}\")\n",
    "print(f\"  Max: {df['doc_length'].max()}\")\n",
    "\n",
    "# Vocabulary size (rough estimate)\n",
    "all_words = ' '.join(df['text']).lower().split()\n",
    "vocab_size = len(set(all_words))\n",
    "print(f\"\\nApproximate vocabulary size: {vocab_size:,} unique words\")\n",
    "print(f\"Total words in corpus: {len(all_words):,}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84049575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize category distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "category_counts.plot(kind='bar', color='steelblue')\n",
    "plt.title('Document Distribution Across Categories', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Category', fontsize=12)\n",
    "plt.ylabel('Number of Documents', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3709e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample documents from different categories\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE DOCUMENTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for category in df['category'].unique()[:5]:  # Show up to 5 categories\n",
    "    sample = df[df['category'] == category].iloc[0]\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Category: {category.upper()}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    # Show first 500 characters\n",
    "    text_preview = sample['text'][:500]\n",
    "    if len(sample['text']) > 500:\n",
    "        text_preview += \"...\"\n",
    "    print(text_preview)\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaad1ad",
   "metadata": {},
   "source": [
    "### Diagnostic: Word Frequency Analysis\n",
    "\n",
    "Since our modeling assumptions center relative word frequency and co-occurence, it makes sense to start with understanding what words dominate our corpus. This will reveal our first challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b2bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count word frequencies across entire corpus\n",
    "all_words_lower = ' '.join(df['text']).lower().split()\n",
    "word_freq = Counter(all_words_lower)\n",
    "most_common = word_freq.most_common(30)\n",
    "\n",
    "# Visualize\n",
    "words, counts = zip(*most_common)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(range(len(words)), counts, color='coral')\n",
    "plt.xticks(range(len(words)), words, rotation=45, ha='right')\n",
    "plt.title('Top 30 Most Frequent Words in Corpus', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOBSERVATION: The most common words are 'the', 'to', 'a', 'of', 'and', etc.\")\n",
    "print(\"These appear in every document regardless of topic. (Often called 'stopwords' because they are uninformative and should probably be removed.)\")\n",
    "print(\"\\nWe need to find what makes documents DIFFERENT, not what they share.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc07bb9c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Iteration 1 - Baseline with Bag-of-Words + K-Means\n",
    "We start with a simple representation: we translate each document into a vector that keeps track of the number of times each word occurs in the document. This removes all information about word-order and bundles everything together.\n",
    "\n",
    "For the clustering algorithm we start with **k-means** that tries to find a predefined number (e.g. 5) *central points* (called centroids) in the data and then assigns each document to it's nearest centroid.\n",
    "\n",
    "This will serve as our baseline.\n",
    "\n",
    "**What we expect**: Poor results because we're counting all words equally, including uninformative ones, and because we assign each document a single \"topic\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3612452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bag-of-Words representation\n",
    "print(\"Creating Bag-of-Words representation...\")\n",
    "\n",
    "# CountVectorizer converts text to word count matrix\n",
    "# We deliberately KEEP stopwords to see the problem\n",
    "vectorizer_bow = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    min_df=2,\n",
    "    max_features=5000,\n",
    "    #stop_words='english'  # Used later\n",
    "    stop_words=None # Note: NO stop_words parameter - we keep everything!\n",
    ")\n",
    "\n",
    "# Fit and transform documents\n",
    "X_bow = vectorizer_bow.fit_transform(df['text'])\n",
    "\n",
    "print(f\"✓ Created document-term matrix\")\n",
    "print(f\"  Shape: {X_bow.shape} (documents × vocabulary)\")\n",
    "print(f\"  Vocabulary size: {len(vectorizer_bow.get_feature_names_out())}\")\n",
    "print(f\"  Sparsity: {100 * (1 - X_bow.nnz / (X_bow.shape[0] * X_bow.shape[1])):.1f}% of entries are zero\")\n",
    "print(f\"\\n  Note: Each document is now a vector of {X_bow.shape[1]} word counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd76980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means clustering\n",
    "print(\"Applying K-Means clustering...\")\n",
    "\n",
    "n_clusters = 5  # We know there are 5 categories in BBC News\n",
    "\n",
    "kmeans_bow = KMeans(\n",
    "    n_clusters=n_clusters,\n",
    "    random_state=42,\n",
    "    n_init=10,  # Run 10 times with different initializations, keep best\n",
    "    max_iter=300\n",
    ")\n",
    "\n",
    "# Fit and predict cluster labels\n",
    "clusters_bow = kmeans_bow.fit_predict(X_bow)\n",
    "\n",
    "print(f\"✓ K-Means clustering complete\")\n",
    "print(f\"  Number of clusters: {n_clusters}\")\n",
    "print(f\"  Iterations to converge: {kmeans_bow.n_iter_}\")\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df['cluster_bow'] = clusters_bow\n",
    "\n",
    "# Show cluster sizes\n",
    "print(f\"\\n  Cluster sizes:\")\n",
    "for i in range(n_clusters):\n",
    "    count = (clusters_bow == i).sum()\n",
    "    print(f\"    Cluster {i}: {count} documents ({100*count/len(df):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81150bf4",
   "metadata": {},
   "source": [
    "Did this give us a nice clustering? How can we tell? \n",
    "\n",
    "**Let's investigate!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f0ebbf",
   "metadata": {},
   "source": [
    "### Diagnostic 1: Top Words Per Cluster\n",
    "\n",
    "Now we'll try to gauge the quality of our clustering through a series of diagnostics that will tell us different important characteristics of our clustering.\n",
    "\n",
    "First let's see what words characterize each cluster. This will reveal a major problem with our baseline approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b1d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top words for each cluster\n",
    "def get_top_words_per_cluster(X, vectorizer, cluster_labels, n_words=10):\n",
    "    \"\"\"\n",
    "    Find the most important words for each cluster by averaging\n",
    "    word counts across all documents in the cluster.\n",
    "    \"\"\"\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    n_clusters = len(np.unique(cluster_labels))\n",
    "    \n",
    "    top_words = {}\n",
    "    \n",
    "    for cluster_id in range(n_clusters):\n",
    "        # Get all documents in this cluster\n",
    "        cluster_mask = cluster_labels == cluster_id\n",
    "        cluster_docs = X[cluster_mask]\n",
    "        \n",
    "        # Average word counts across cluster\n",
    "        avg_counts = np.asarray(cluster_docs.mean(axis=0)).flatten()\n",
    "        \n",
    "        # Get top words\n",
    "        top_indices = avg_counts.argsort()[-n_words:][::-1]\n",
    "        top_words[cluster_id] = [(feature_names[i], avg_counts[i]) for i in top_indices]\n",
    "    \n",
    "    return top_words\n",
    "\n",
    "# Get top words\n",
    "top_words_bow = get_top_words_per_cluster(X_bow, vectorizer_bow, clusters_bow, n_words=10)\n",
    "\n",
    "# Display\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP 10 WORDS PER CLUSTER (Bag-of-Words + K-Means)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for cluster_id, words in top_words_bow.items():\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    word_list = [f\"{word} ({count:.2f})\" for word, count in words]\n",
    "    print(f\"  {', '.join(word_list)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROBLEM: Top words are stopwords ('the', 'to', 'a', 'of')!\")\n",
    "print(\"These appear everywhere and tell us nothing about topics.\")\n",
    "print(\"\\nWe need better weighting that downweights common words.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773fd372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top words for each cluster\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for cluster_id in range(n_clusters):\n",
    "    words, counts = zip(*top_words_bow[cluster_id])\n",
    "    \n",
    "    axes[cluster_id].barh(range(len(words)), counts, color='skyblue')\n",
    "    axes[cluster_id].set_yticks(range(len(words)))\n",
    "    axes[cluster_id].set_yticklabels(words)\n",
    "    axes[cluster_id].set_xlabel('Average Count', fontsize=10)\n",
    "    axes[cluster_id].set_title(f'Cluster {cluster_id} Top Words', fontsize=11, fontweight='bold')\n",
    "    axes[cluster_id].invert_yaxis()\n",
    "    axes[cluster_id].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.suptitle('Top Words Per Cluster (Bag-of-Words) - Dominated by Stopwords', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1926de81",
   "metadata": {},
   "source": [
    "Note how there is a huge overlap in the most frequent words across clusters and what seems to separate them is the absolute number of these words in each text - we accidentally clustered based on text-length, not topic!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51537cf5",
   "metadata": {},
   "source": [
    "### Diagnostic 2: Cluster Quality Metrics\n",
    "\n",
    "We'll evaluate clusters using several metrics:\n",
    "\n",
    "1. **Cluster balance**: Are clusters roughly equal in size, or is everything in one cluster?\n",
    "2. **Cluster purity**: If we know true categories, how homogeneous are clusters?\n",
    "3. **Top word distinctiveness**: Do clusters have unique characteristic words (do they represent distinct topics)?\n",
    "\n",
    "These metrics tell us if clusters are meaningful, not just geometrically separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7831e437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cluster quality metrics\n",
    "print(\"Calculating cluster quality metrics...\")\n",
    "\n",
    "# 1. Cluster balance (entropy of size distribution)\n",
    "from scipy.stats import entropy\n",
    "\n",
    "cluster_sizes = np.bincount(clusters_bow)\n",
    "cluster_balance = entropy(cluster_sizes / cluster_sizes.sum()) / np.log(n_clusters)\n",
    "\n",
    "# 2. Cluster purity (if we have true labels)\n",
    "def cluster_purity(labels_true, labels_pred):\n",
    "    \"\"\"Calculate purity: fraction of dominant class in each cluster\"\"\"\n",
    "    contingency = pd.crosstab(labels_pred, labels_true)\n",
    "    return (contingency.max(axis=1).sum() / len(labels_true))\n",
    "\n",
    "purity_bow = cluster_purity(df['category'], clusters_bow)\n",
    "\n",
    "# 3. Top word distinctiveness (how unique are top words?)\n",
    "def top_word_overlap(top_words_dict, n_words=10):\n",
    "    \"\"\"Calculate average overlap between cluster top words\"\"\"\n",
    "    all_top_words = [set([w for w, _ in words[:n_words]]) for words in top_words_dict.values()]\n",
    "    overlaps = []\n",
    "    for i in range(len(all_top_words)):\n",
    "        for j in range(i+1, len(all_top_words)):\n",
    "            overlap = len(all_top_words[i] & all_top_words[j]) / n_words\n",
    "            overlaps.append(overlap)\n",
    "    return np.mean(overlaps)\n",
    "\n",
    "word_overlap_bow = top_word_overlap(top_words_bow)\n",
    "\n",
    "print(f\"✓ Cluster quality analysis complete\")\n",
    "print(f\"\\n  Cluster balance: {cluster_balance:.3f} (1.0 = perfectly balanced)\")\n",
    "print(f\"  Cluster purity: {purity_bow:.3f} (1.0 = perfect match with categories)\")\n",
    "print(f\"  Top word overlap: {word_overlap_bow:.3f} (0.0 = completely distinct)\")\n",
    "print(f\"\\n  Interpretation:\")\n",
    "if cluster_balance < 0.9:\n",
    "    print(f\"    ⚠️ Unbalanced: Some clusters dominate\")\n",
    "else:\n",
    "    print(f\"    ✅ Balanced: Clusters are fairly even in size\")\n",
    "if purity_bow < 0.5:\n",
    "    print(f\"    ⚠️ Low purity: Clusters don't match known categories well\")\n",
    "else:\n",
    "    print(f\"    ✅ High purity: Clusters match known categories well\")\n",
    "if word_overlap_bow > 0.3:\n",
    "    print(f\"    ⚠️ High overlap: Clusters share many top words (not distinctive)\")\n",
    "else:\n",
    "    print(f\"    ✅ Low overlap: Clusters are lexically distinct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440e8b0a",
   "metadata": {},
   "source": [
    "**Before moving on**: Try to go back and add stop_words='english' to the CountVectorizer to remove common words and improve clustering.\n",
    "\n",
    "We'll try to visualize the size distribution and purity to get a feel for the clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20908e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster balance and purity\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Cluster size distribution\n",
    "cluster_sizes_pct = cluster_sizes / cluster_sizes.sum() * 100\n",
    "ax1.bar(range(n_clusters), cluster_sizes_pct, color='steelblue', edgecolor='black')\n",
    "ax1.axhline(y=100/n_clusters, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Perfect balance ({100/n_clusters:.1f}%)')\n",
    "ax1.set_xlabel('Cluster ID', fontsize=12)\n",
    "ax1.set_ylabel('Percentage of Documents', fontsize=12)\n",
    "ax1.set_title(f'Cluster Size Distribution (Balance: {cluster_balance:.3f})', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Cluster composition (true categories)\n",
    "contingency = pd.crosstab(clusters_bow, df['category'], normalize='index') * 100\n",
    "contingency.plot(kind='bar', stacked=True, ax=ax2, colormap='Set3', legend=False)\n",
    "ax2.set_xlabel('Cluster ID', fontsize=12)\n",
    "ax2.set_ylabel('Percentage', fontsize=12)\n",
    "ax2.set_title(f'Cluster Composition by Category (Purity: {purity_bow:.3f})', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax2.legend(title='Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=0)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOBSERVATION: Clusters are unbalanced and have low purity.\")\n",
    "print(\"This confirms that BoW with stopwords doesn't create meaningful groupings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623afc31",
   "metadata": {},
   "source": [
    "### Diagnostic 3: 2D Visualization with PCA\n",
    "\n",
    "We can use **principle components analysis** to project our documents into a 2D space to see, if the clusters are geometrically separate. \n",
    "\n",
    "**Note**: We lose A LOT of information doing this, so be wary of reading too much information into this plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cd7cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to 2D using PCA\n",
    "print(\"Reducing to 2D with PCA...\")\n",
    "pca_bow = PCA(n_components=2, random_state=42)\n",
    "X_bow_2d = pca_bow.fit_transform(X_bow.toarray())\n",
    "\n",
    "print(f\"✓ PCA complete\")\n",
    "print(f\"  Variance explained: {100*pca_bow.explained_variance_ratio_.sum():.1f}%\")\n",
    "print(f\"  (This is how much information we retained in 2D)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007fea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters in 2D\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot each cluster with different color\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, n_clusters))\n",
    "\n",
    "for cluster_id in range(n_clusters):\n",
    "    mask = clusters_bow == cluster_id\n",
    "    plt.scatter(X_bow_2d[mask, 0], X_bow_2d[mask, 1], \n",
    "                c=[colors[cluster_id]], label=f'Cluster {cluster_id}',\n",
    "                alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "plt.xlabel('First Principal Component', fontsize=12)\n",
    "plt.ylabel('Second Principal Component', fontsize=12)\n",
    "plt.title('Document Clusters in 2D (Bag-of-Words + K-Means)\\nNo Distinct Cluster Boundaries', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10, loc='best')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOBSERVATION: Clusters do not have distinct boundaries in 2D projection.\")\n",
    "print(\"Documents from different clusters are mixed together.\")\n",
    "print(\"\\nThis confirms poor cluster separation due to uninformative features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c8f368",
   "metadata": {},
   "source": [
    "### Reflection: What Did We Learn?\n",
    "\n",
    "**Problems with Iteration 1**:\n",
    "1. ❌ Top words are stopwords (\"the\", \"to\", \"a\")\n",
    "2. ❌ Clusters are unbalanced (some very large, some very small)\n",
    "3. ❌ Low purity (clusters don't match known categories)\n",
    "4. ❌ High word overlap (clusters aren't distinctive) unless stopwords are removed.\n",
    "\n",
    "**Why?** Bag-of-Words treats all words equally. Common words dominate, making all documents look similar. We risk accidentally clustering based on the absolute count of words rather than their relative frequency and co-occurence.\n",
    "\n",
    "**Next step**: We'll start by improving our **representation** to *TF-IDF* which automatically downweights common words and upweights distinctive ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b20d03b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Iteration 2 - Better Representation with TF-IDF + K-Means\n",
    "\n",
    "**What we're changing**: ONLY the data representation (Bag-of-Words → TF-IDF)\n",
    "\n",
    "**What stays the same**: K-Means algorithm, number of clusters (k=5), all other parameters\n",
    "\n",
    "**What is TF-IDF?**\n",
    "\n",
    "**TF-IDF** = Term Frequency × Inverse Document Frequency\n",
    "\n",
    "- **TF (Term Frequency)**: How often does this word appear in this document?\n",
    "- **IDF (Inverse Document Frequency)**: How rare is this word across all documents?\n",
    "\n",
    "**The key insight**: \n",
    "- A word appearing in EVERY document (like \"the\") gets LOW weight\n",
    "- A word appearing in only SPORTS articles (like \"football\") gets HIGH weight\n",
    "\n",
    "This automatically downweights common words and upweights distinctive ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfe1938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF representation\n",
    "print(\"Creating TF-IDF representation...\")\n",
    "\n",
    "# TfidfVectorizer: same as CountVectorizer but applies TF-IDF weighting\n",
    "vectorizer_tfidf = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    min_df=2,\n",
    "    max_features=5000,\n",
    "    stop_words='english' # If you want you can try without removing stopwords first\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(df['text'])\n",
    "\n",
    "print(f\"✓ Created TF-IDF matrix\")\n",
    "print(f\"  Shape: {X_tfidf.shape} (same as before)\")\n",
    "print(f\"  Vocabulary size: {len(vectorizer_tfidf.get_feature_names_out())}\")\n",
    "print(f\"\\n  Key difference: Values are now TF-IDF scores, not raw counts\")\n",
    "print(f\"  Common words get low scores, rare distinctive words get high scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b4272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means clustering (SAME algorithm, different representation)\n",
    "print(\"Applying K-Means clustering to TF-IDF vectors...\")\n",
    "\n",
    "kmeans_tfidf = KMeans(\n",
    "    n_clusters=n_clusters,\n",
    "    random_state=42,\n",
    "    n_init=10,\n",
    "    max_iter=300\n",
    ")\n",
    "\n",
    "clusters_tfidf = kmeans_tfidf.fit_predict(X_tfidf)\n",
    "\n",
    "print(f\"✓ K-Means clustering complete\")\n",
    "print(f\"  Iterations to converge: {kmeans_tfidf.n_iter_}\")\n",
    "\n",
    "# Add to dataframe\n",
    "df['cluster_tfidf'] = clusters_tfidf\n",
    "\n",
    "# Show cluster sizes\n",
    "print(f\"\\n  Cluster sizes:\")\n",
    "for i in range(n_clusters):\n",
    "    count = (clusters_tfidf == i).sum()\n",
    "    print(f\"    Cluster {i}: {count} documents ({100*count/len(df):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d102dd",
   "metadata": {},
   "source": [
    "### Visualizing the TF-IDF Representation\n",
    "\n",
    "Now let's see how TF-IDF changes the representation. The key difference: common words that occur at about similar rates across documents get downweighted, distinctive words that only occur in some documents get upweighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d30c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the SAME document with TF-IDF representation\n",
    "sample_idx = 0\n",
    "sample_vector_tfidf = X_tfidf[sample_idx].toarray().flatten()\n",
    "\n",
    "# Get feature names\n",
    "feature_names_tfidf = vectorizer_tfidf.get_feature_names_out()\n",
    "\n",
    "# Find non-zero entries\n",
    "nonzero_indices_tfidf = np.nonzero(sample_vector_tfidf)[0]\n",
    "word_scores = [(feature_names_tfidf[i], sample_vector_tfidf[i]) for i in nonzero_indices_tfidf]\n",
    "word_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SAME DOCUMENT (TF-IDF Representation)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nOriginal text (first 300 characters):\")\n",
    "print(f\"{df['text'][sample_idx][:300]}...\")\n",
    "print(f\"\\nDocument as TF-IDF vector:\")\n",
    "print(f\"  Vector length: {len(sample_vector_tfidf)}\")\n",
    "print(f\"  Non-zero entries: {len(nonzero_indices_tfidf)}\")\n",
    "print(f\"\\nTop 20 words by TF-IDF score:\")\n",
    "for word, score in word_scores[:20]:\n",
    "    print(f\"  '{word}': {score:.4f}\")\n",
    "print(\"\\n✓ Notice: Top words seem topical for the article (without stopwords removed, notice how irrelevant words are no longer COMPELTELY dominant, but others are there as well).\")\n",
    "print(\"✓ Distinctive words have HIGH scores\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cbec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize TF-IDF scores for sample document\n",
    "top_n = 30\n",
    "words_tfidf, scores_tfidf = zip(*word_scores[:top_n])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.barh(range(len(words_tfidf)), scores_tfidf, color='lightgreen')\n",
    "plt.yticks(range(len(words_tfidf)), words_tfidf)\n",
    "plt.xlabel('TF-IDF Score (higher = more distinctive)', fontsize=12)\n",
    "plt.ylabel('Word', fontsize=12)\n",
    "plt.title(f'TF-IDF: Top {top_n} Words in Sample Document\\n✅ Distinctive words are emphasized!', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ IMPROVEMENT: Top words are now more topic-specific and meaningful!\")\n",
    "print(\"Generic words have been automatically downweighted (though it still makes sense to remove them to improve performance further).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c624666",
   "metadata": {},
   "source": [
    "### Diagnostic 1: Top Words Per Cluster (TF-IDF)\n",
    "\n",
    "Now let's see if the top words are more meaningful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c871be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words for TF-IDF clusters\n",
    "top_words_tfidf = get_top_words_per_cluster(X_tfidf, vectorizer_tfidf, clusters_tfidf, n_words=10)\n",
    "\n",
    "# Display\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP 10 WORDS PER CLUSTER (TF-IDF + K-Means)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for cluster_id, words in top_words_tfidf.items():\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    word_list = [f\"{word} ({score:.3f})\" for word, score in words]\n",
    "    print(f\"  {', '.join(word_list)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ IMPROVEMENT: Top words are now meaningful and topic-specific!\")\n",
    "print(\"Notice words like 'film', 'game', 'government', 'technology', etc.\")\n",
    "print(\"These actually tell us what each cluster is about.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fc28e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top words for each cluster\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for cluster_id in range(n_clusters):\n",
    "    words, scores = zip(*top_words_tfidf[cluster_id])\n",
    "    \n",
    "    axes[cluster_id].barh(range(len(words)), scores, color='lightgreen')\n",
    "    axes[cluster_id].set_yticks(range(len(words)))\n",
    "    axes[cluster_id].set_yticklabels(words)\n",
    "    axes[cluster_id].set_xlabel('Average TF-IDF Score', fontsize=10)\n",
    "    axes[cluster_id].set_title(f'Cluster {cluster_id} Top Words', fontsize=11, fontweight='bold')\n",
    "    axes[cluster_id].invert_yaxis()\n",
    "    axes[cluster_id].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.suptitle('Top Words Per Cluster (TF-IDF) - Much More Meaningful!', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f144c8f1",
   "metadata": {},
   "source": [
    "### Diagnostic 2: Cluster Quality Metrics (TF-IDF)\n",
    "\n",
    "We'll analyse the quality with the same metrics as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee878f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cluster quality metrics for TF-IDF\n",
    "print(\"Calculating cluster quality metrics for TF-IDF clustering...\")\n",
    "\n",
    "# Cluster balance (fixed formula)\n",
    "cluster_sizes_tfidf = np.bincount(clusters_tfidf)\n",
    "cluster_balance_tfidf = entropy(cluster_sizes_tfidf / cluster_sizes_tfidf.sum()) / np.log(n_clusters)\n",
    "\n",
    "# Cluster purity\n",
    "purity_tfidf = cluster_purity(df['category'], clusters_tfidf)\n",
    "\n",
    "# Top word distinctiveness\n",
    "word_overlap_tfidf = top_word_overlap(top_words_tfidf)\n",
    "\n",
    "print(f\"✓ Cluster quality analysis complete\")\n",
    "print(f\"\\n  Cluster balance: {cluster_balance_tfidf:.3f} (previous: {cluster_balance:.3f})\")\n",
    "print(f\"  Cluster purity: {purity_tfidf:.3f} (previous: {purity_bow:.3f})\")\n",
    "print(f\"  Top word overlap: {word_overlap_tfidf:.3f} (previous: {word_overlap_bow:.3f})\")\n",
    "print(f\"\\n  Improvements:\")\n",
    "print(f\"    Balance: {cluster_balance_tfidf - cluster_balance:+.3f}\")\n",
    "print(f\"    Purity: {purity_tfidf - purity_bow:+.3f}\")\n",
    "print(f\"    Distinctiveness: {word_overlap_bow - word_overlap_tfidf:+.3f} (lower overlap is better)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c696ba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare cluster quality metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Row 1: Cluster size distributions\n",
    "cluster_sizes_bow_pct = cluster_sizes / cluster_sizes.sum() * 100\n",
    "cluster_sizes_tfidf_pct = cluster_sizes_tfidf / cluster_sizes_tfidf.sum() * 100\n",
    "\n",
    "axes[0, 0].bar(range(n_clusters), cluster_sizes_bow_pct, color='steelblue', edgecolor='black')\n",
    "axes[0, 0].axhline(y=100/n_clusters, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Cluster ID', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Percentage of Documents', fontsize=11)\n",
    "axes[0, 0].set_title(f'BoW: Balance = {cluster_balance:.3f}', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "axes[0, 1].bar(range(n_clusters), cluster_sizes_tfidf_pct, color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].axhline(y=100/n_clusters, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Cluster ID', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Percentage of Documents', fontsize=11)\n",
    "axes[0, 1].set_title(f'TF-IDF: Balance = {cluster_balance_tfidf:.3f}', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Row 2: Cluster purity (if we have true labels)\n",
    "contingency_bow = pd.crosstab(clusters_bow, df['category'], normalize='index') * 100\n",
    "contingency_tfidf = pd.crosstab(clusters_tfidf, df['category'], normalize='index') * 100\n",
    "\n",
    "contingency_bow.plot(kind='bar', stacked=True, ax=axes[1, 0], colormap='Set3', legend=False)\n",
    "axes[1, 0].set_xlabel('Cluster ID', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Percentage', fontsize=11)\n",
    "axes[1, 0].set_title(f'BoW: Purity = {purity_bow:.3f}', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xticklabels(axes[1, 0].get_xticklabels(), rotation=0)\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "contingency_tfidf.plot(kind='bar', stacked=True, ax=axes[1, 1], colormap='Set3', legend=True)\n",
    "axes[1, 1].set_xlabel('Cluster ID', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Percentage', fontsize=11)\n",
    "axes[1, 1].set_title(f'TF-IDF: Purity = {purity_tfidf:.3f}', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].legend(title='Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[1, 1].set_xticklabels(axes[1, 1].get_xticklabels(), rotation=0)\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Cluster Quality Comparison: TF-IDF Shows Clear Improvement', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Purity and balance both increase drastically with TF-IDF compared to BoW.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1660b09",
   "metadata": {},
   "source": [
    "### Diagnostic 3: 2D Visualization with PCA\n",
    "\n",
    "Again we'll try to visualize the representation in 2d to see if the clusters are better separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f2d5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to 2D using PCA\n",
    "print(\"Reducing to 2D with PCA...\")\n",
    "pca_tfidf = PCA(n_components=2, random_state=42)\n",
    "X_tfidf_2d = pca_tfidf.fit_transform(X_tfidf.toarray())\n",
    "\n",
    "print(f\"✓ PCA complete\")\n",
    "print(f\"  Variance explained: {100*pca_tfidf.explained_variance_ratio_.sum():.1f}%\")\n",
    "print(f\"  (This is how much information we retained in 2D)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f693b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters in 2D\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot each cluster with different color\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, n_clusters))\n",
    "\n",
    "for cluster_id in range(n_clusters):\n",
    "    mask = clusters_tfidf == cluster_id\n",
    "    plt.scatter(X_tfidf_2d[mask, 0], X_tfidf_2d[mask, 1], \n",
    "                c=[colors[cluster_id]], label=f'Cluster {cluster_id}',\n",
    "                alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "plt.xlabel('First Principal Component', fontsize=12)\n",
    "plt.ylabel('Second Principal Component', fontsize=12)\n",
    "plt.title('Document Clusters in 2D (TF-IDF + K-Means)\\nCluster boundaries become clearer', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10, loc='best')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOBSERVATION: Clusters are more distinct, although still with a large overlap in 2D projection.\")\n",
    "print(\"\\nOur clustering seems to have improved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db6222c",
   "metadata": {},
   "source": [
    " ### Reflection: What Did We Learn?\n",
    "\n",
    "**Improvements from Iteration 2**:\n",
    "1. ✅ Top words are more meaningful and topic-specific\n",
    "2. ✅ Clusters are more balanced (better size distribution)\n",
    "3. ✅ Higher purity (clusters match known categories)\n",
    "4. ✅ Lower word overlap (clusters are more distinctive)\n",
    "\n",
    "**Remaining problems**:\n",
    "1. ❌ K-Means forces hard assignments (one cluster per document)\n",
    "2. ❌ Many documents discuss multiple topics\n",
    "3. ❌ Real articles often span categories (e.g., \"technology in politics\")\n",
    "\n",
    "**Next step**: Use LDA for soft clustering where documents can belong to multiple topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d505fc0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Iteration 3 - Soft Clustering with LDA\n",
    "\n",
    "**What we're changing**: ONLY the algorithm (K-Means → LDA)\n",
    "\n",
    "**What stays the same**: Number of topics (k=5)\n",
    "\n",
    "**Important note about representation**: We need to go back to count-based features (Bag-of-Words) for LDA, not TF-IDF!\n",
    "\n",
    "**What is LDA?**\n",
    "\n",
    "**LDA** = Latent Dirichlet Allocation (a probabilistic topic model)\n",
    "\n",
    "**Key differences from K-Means**:\n",
    "- **Soft clustering**: Each document is a MIXTURE of topics (e.g., 60% politics, 30% technology, 10% business)\n",
    "- **Interpretable topics**: Each topic is a distribution over words\n",
    "- **Generative model**: Assumes documents are created by mixing topics\n",
    "\n",
    "**The key insight**: Real documents often discuss multiple themes. LDA captures this naturally!\n",
    "\n",
    "**Why not TF-IDF?** LDA is a probabilistic model that assumes word counts follow a multinomial distribution. TF-IDF values are weighted scores, not counts, which violates LDA's mathematical assumptions. Using TF-IDF with LDA can lead to poor topic quality and convergence issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9a4e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bag-of-Words representation for LDA\n",
    "print(\"Creating Bag-of-Words representation for LDA...\")\n",
    "\n",
    "# LDA requires count-based features, not TF-IDF weighted scores\n",
    "# We'll use CountVectorizer with stopwords removed (building on Iteration 2's insight)\n",
    "vectorizer_lda = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    min_df=2,\n",
    "    max_features=5000,\n",
    "    stop_words='english'  # Remove stopwords based on Iteration 2 learning\n",
    ")\n",
    "\n",
    "# Fit and transform documents\n",
    "X_lda = vectorizer_lda.fit_transform(df['text'])\n",
    "\n",
    "print(f\"✓ Created document-term matrix for LDA\")\n",
    "print(f\"  Shape: {X_lda.shape} (documents × vocabulary)\")\n",
    "print(f\"  Vocabulary size: {len(vectorizer_lda.get_feature_names_out())}\")\n",
    "print(f\"  Sparsity: {100 * (1 - X_lda.nnz / (X_lda.shape[0] * X_lda.shape[1])):.1f}% of entries are zero\")\n",
    "print(f\"\\n  Note: Using raw counts (not TF-IDF) because LDA is a probabilistic model\")\n",
    "print(f\"  But we keep the stopword removal insight from Iteration 2!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9197d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LDA topic modeling\n",
    "print(\"Applying LDA topic modeling...\")\n",
    "\n",
    "n_topics = 5  # Same as number of clusters before\n",
    "\n",
    "lda_model = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    random_state=42,\n",
    "    max_iter=20,\n",
    "    learning_method='batch',\n",
    "    n_jobs=-1  # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Fit model and get document-topic distributions\n",
    "doc_topic_dist = lda_model.fit_transform(X_lda)\n",
    "\n",
    "print(f\"✓ LDA modeling complete\")\n",
    "print(f\"  Number of topics: {n_topics}\")\n",
    "print(f\"  Iterations: {lda_model.n_iter_}\")\n",
    "print(f\"  Final perplexity: {lda_model.perplexity(X_lda):.2f} (lower is better)\")\n",
    "\n",
    "# Get dominant topic for each document (for comparison with K-Means)\n",
    "dominant_topics = doc_topic_dist.argmax(axis=1)\n",
    "df['topic_lda'] = dominant_topics\n",
    "\n",
    "print(f\"\\n  Topic sizes (by dominant topic):\")\n",
    "for i in range(n_topics):\n",
    "    count = (dominant_topics == i).sum()\n",
    "    print(f\"    Topic {i}: {count} documents ({100*count/len(df):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7b1550",
   "metadata": {},
   "source": [
    "### Diagnostic 1: Topic-Word Distributions\n",
    "\n",
    "Let's see what words characterize each topic. Unlike K-Means, LDA gives us probability distributions over words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2188708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top words for each topic\n",
    "def get_top_words_per_topic(model, vectorizer, n_words=10):\n",
    "    \"\"\"\n",
    "    Get top words for each topic based on word probabilities.\n",
    "    \"\"\"\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    top_words = {}\n",
    "    \n",
    "    for topic_id, topic_dist in enumerate(model.components_):\n",
    "        # Get top word indices\n",
    "        top_indices = topic_dist.argsort()[-n_words:][::-1]\n",
    "        # Get words and their probabilities\n",
    "        top_words[topic_id] = [(feature_names[i], topic_dist[i]) for i in top_indices]\n",
    "    \n",
    "    return top_words\n",
    "\n",
    "# Get top words for each topic (using TF-IDF vocabulary)\n",
    "top_words_lda = get_top_words_per_topic(lda_model, vectorizer_lda, n_words=10)\n",
    "\n",
    "# Display\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP 10 WORDS PER TOPIC (LDA with Bag-of-Words)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for topic_id, words in top_words_lda.items():\n",
    "    print(f\"\\nTopic {topic_id}:\")\n",
    "    word_list = [f\"{word} ({prob:.3f})\" for word, prob in words]\n",
    "    print(f\"  {', '.join(word_list)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ OBSERVATION: Topics are coherent and interpretable!\")\n",
    "print(\"Each topic has a clear theme (e.g., sports, politics, technology).\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0ba636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top words for each topic\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for topic_id in range(n_topics):\n",
    "    words, scores = zip(*top_words_lda[topic_id])\n",
    "    \n",
    "    axes[topic_id].barh(range(len(words)), scores, color='mediumpurple')\n",
    "    axes[topic_id].set_yticks(range(len(words)))\n",
    "    axes[topic_id].set_yticklabels(words)\n",
    "    axes[topic_id].set_xlabel('Probability in Topic', fontsize=10)\n",
    "    axes[topic_id].set_title(f'Topic {topic_id} Top Words', fontsize=11, fontweight='bold')\n",
    "    axes[topic_id].invert_yaxis()\n",
    "    axes[topic_id].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.suptitle('Top Words Per Topic (LDA) - Coherent Themes!', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a737ea6",
   "metadata": {},
   "source": [
    "### Diagnostic 2: Cluster Quality Metrics (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf05a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cluster quality metrics for LDA\n",
    "print(\"Calculating cluster quality metrics for LDA...\")\n",
    "\n",
    "# Cluster balance (using dominant topics)\n",
    "topic_sizes = np.bincount(dominant_topics)\n",
    "cluster_balance_lda = entropy(topic_sizes / topic_sizes.sum()) / np.log(n_topics)\n",
    "\n",
    "# Cluster purity\n",
    "purity_lda = cluster_purity(df['category'], dominant_topics)\n",
    "\n",
    "# Top word distinctiveness\n",
    "word_overlap_lda = top_word_overlap(top_words_lda)\n",
    "\n",
    "print(f\"✓ Cluster quality analysis complete\")\n",
    "print(f\"\\n  Cluster balance: {cluster_balance_lda:.3f}\")\n",
    "print(f\"  Cluster purity: {purity_lda:.3f}\")\n",
    "print(f\"  Top word overlap: {word_overlap_lda:.3f}\")\n",
    "print(f\"\\n  Comparison:\")\n",
    "print(f\"    BoW:    balance={cluster_balance:.3f}, purity={purity_bow:.3f}\")\n",
    "print(f\"    TF-IDF: balance={cluster_balance_tfidf:.3f}, purity={purity_tfidf:.3f}\")\n",
    "print(f\"    LDA:    balance={cluster_balance_lda:.3f}, purity={purity_lda:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c366ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all methods\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
    "# Row 1: Cluster size distributions\n",
    "methods = ['BoW\\n+K-Means', 'TF-IDF\\n+K-Means', 'BoW\\n+LDA']\n",
    "balances = [cluster_balance, cluster_balance_tfidf, cluster_balance_lda]\n",
    "size_dists = [\n",
    "    cluster_sizes / cluster_sizes.sum() * 100,\n",
    "    cluster_sizes_tfidf / cluster_sizes_tfidf.sum() * 100,\n",
    "    topic_sizes / topic_sizes.sum() * 100\n",
    "]\n",
    "colors_bar = ['steelblue', 'lightgreen', 'mediumpurple']\n",
    "\n",
    "for i, (method, balance, sizes, color) in enumerate(zip(methods, balances, size_dists, colors_bar)):\n",
    "    axes[0, i].bar(range(n_clusters), sizes, color=color, edgecolor='black')\n",
    "    axes[0, i].axhline(y=100/n_clusters, color='red', linestyle='--', linewidth=2)\n",
    "    axes[0, i].set_xlabel('Cluster/Topic ID', fontsize=9)\n",
    "    axes[0, i].set_ylabel('Percentage', fontsize=9)\n",
    "    axes[0, i].set_title(f'{method}\\nBalance = {balance:.3f}', fontsize=10, fontweight='bold')\n",
    "    axes[0, i].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Row 2: Cluster purity (composition)\n",
    "purities = [purity_bow, purity_tfidf, purity_lda]\n",
    "cluster_labels_all = [clusters_bow, clusters_tfidf, dominant_topics]\n",
    "contingencies = [\n",
    "    pd.crosstab(labels, df['category'], normalize='index') * 100\n",
    "    for labels in cluster_labels_all\n",
    "]\n",
    "\n",
    "for i, (method, purity, cont, color) in enumerate(zip(methods, purities, contingencies, colors_bar)):\n",
    "    cont.plot(kind='bar', stacked=True, ax=axes[1, i], colormap='Set3', legend=(i==2))\n",
    "    axes[1, i].set_xlabel('Cluster/Topic ID', fontsize=9)\n",
    "    axes[1, i].set_ylabel('Percentage', fontsize=9)\n",
    "    axes[1, i].set_title(f'{method}\\nPurity = {purity:.3f}', fontsize=10, fontweight='bold')\n",
    "    axes[1, i].set_xticklabels(axes[1, i].get_xticklabels(), rotation=0)\n",
    "    axes[1, i].grid(axis='y', alpha=0.3)\n",
    "    if i == 2:\n",
    "        axes[1, i].legend(title='Category', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "\n",
    "plt.suptitle('Progression so far', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Purity and balance both decreases from TF-IDF to LDA, but LDA provides a SOFTER clustering that might better reflects document complexity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7164b92",
   "metadata": {},
   "source": [
    "### BONUS: Soft Clustering in Action\n",
    "\n",
    "Skip this exercise if you lack time.\n",
    "\n",
    "Let's see how LDA's soft clustering handles documents that span multiple topics. Unlike K-Means which forces a hard assignment, LDA gives us a probability distribution over topics for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2eeb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find documents with ambiguous topic assignments\n",
    "# These are documents where the dominant topic has low probability (< 0.5)\n",
    "# or where multiple topics have similar probabilities\n",
    "\n",
    "# Calculate topic entropy for each document (higher = more ambiguous)\n",
    "from scipy.stats import entropy as scipy_entropy\n",
    "\n",
    "doc_topic_entropy = np.array([scipy_entropy(dist) for dist in doc_topic_dist])\n",
    "\n",
    "# Get max topic probability for each document\n",
    "max_topic_prob = doc_topic_dist.max(axis=1)\n",
    "\n",
    "# Find ambiguous documents (low max probability or high entropy)\n",
    "ambiguous_mask = (max_topic_prob < 0.5) | (doc_topic_entropy > 1.0)\n",
    "ambiguous_indices = np.where(ambiguous_mask)[0]\n",
    "\n",
    "print(f\"Found {len(ambiguous_indices)} ambiguous documents ({100*len(ambiguous_indices)/len(df):.1f}%)\")\n",
    "print(f\"  Low max probability (< 0.5): {(max_topic_prob < 0.5).sum()}\")\n",
    "print(f\"  High entropy (> 1.0): {(doc_topic_entropy > 1.0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b599dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize topic distribution for a few ambiguous documents\n",
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLES OF SOFT CLUSTERING: AMBIGUOUS DOCUMENTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show 3 most ambiguous documents\n",
    "most_ambiguous_indices = doc_topic_entropy.argsort()[-3:][::-1]\n",
    "\n",
    "for rank, idx in enumerate(most_ambiguous_indices, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example {rank}: Document {idx}\")\n",
    "    print(f\"Category: {df.iloc[idx]['category']}\")\n",
    "    print(f\"Topic entropy: {doc_topic_entropy[idx]:.3f} (higher = more mixed)\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    # Show topic distribution\n",
    "    topic_dist = doc_topic_dist[idx]\n",
    "    print(\"\\nTopic distribution:\")\n",
    "    for topic_id in range(n_topics):\n",
    "        prob = topic_dist[topic_id]\n",
    "        if prob > 0.05:  # Only show topics with > 5% probability\n",
    "            top_words = [w for w, _ in top_words_lda[topic_id][:5]]\n",
    "            print(f\"  Topic {topic_id}: {prob*100:5.1f}% ({', '.join(top_words)})\")\n",
    "    \n",
    "    # Show text preview\n",
    "    print(f\"\\nText preview:\")\n",
    "    print(f\"{df.iloc[idx]['text'][:400]}...\")\n",
    "    \n",
    "    # Compare with K-Means hard assignment\n",
    "    kmeans_cluster = clusters_tfidf[idx]\n",
    "    print(f\"\\nK-Means would assign this to: Cluster {kmeans_cluster} (hard assignment)\")\n",
    "    print(f\"LDA shows it's a mixture of topics (soft assignment)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6157c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of topic probabilities\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Distribution of max topic probabilities\n",
    "axes[0].hist(max_topic_prob, bins=30, color='mediumpurple', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Ambiguous threshold')\n",
    "axes[0].set_xlabel('Maximum Topic Probability', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Documents', fontsize=12)\n",
    "axes[0].set_title('Distribution of Topic Certainty\\n(Lower = More Ambiguous)', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Right: Distribution of topic entropy\n",
    "axes[1].hist(doc_topic_entropy, bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=1.0, color='red', linestyle='--', linewidth=2, label='High entropy threshold')\n",
    "axes[1].set_xlabel('Topic Entropy', fontsize=12)\n",
    "axes[1].set_ylabel('Number of Documents', fontsize=12)\n",
    "axes[1].set_title('Distribution of Topic Mixing\\n(Higher = More Mixed)', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Soft Clustering: Many Documents Span Multiple Topics', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ INSIGHT: {(max_topic_prob < 0.5).sum()} documents ({100*(max_topic_prob < 0.5).sum()/len(df):.1f}%) have max topic probability < 50%\")\n",
    "print(\"These documents might discuss multiple themes - soft clustering is able to capture this.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daa5eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters in 2D\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for cluster_id in range(n_clusters):\n",
    "    mask = clusters_tfidf == cluster_id\n",
    "    plt.scatter(X_tfidf_2d[mask, 0], X_tfidf_2d[mask, 1],\n",
    "                c=[colors[cluster_id]], label=f'Cluster {cluster_id}',\n",
    "                alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "plt.xlabel('First Principal Component', fontsize=12)\n",
    "plt.ylabel('Second Principal Component', fontsize=12)\n",
    "plt.title('Document Clusters in 2D (TF-IDF + K-Means)\\nBetter Separation', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10, loc='best')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ IMPROVEMENT: Clusters are more clearly separated than with Bag-of-Words. (More of a sanity check than anything else)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eae85ea",
   "metadata": {},
   "source": [
    "### Reflection: What Did We Learn?\n",
    "\n",
    "**Problems with lexical representations (BoW/TF-IDF)**:\n",
    "1. ❌ Ignore word order (\"dog bites man\" vs \"man bites dog\" look similar)\n",
    "2. ❌ No semantic understanding (synonyms treated as different words)\n",
    "3. ❌ Sparse, high-dimensional vectors (5000+ dimensions)\n",
    "4. ❌ Can't capture context or nuance\n",
    "\n",
    "**Improvements so far**:\n",
    "1. ✅ TF-IDF better than BoW (downweights common words)\n",
    "2. ✅ LDA provides soft clustering (documents can have multiple topics)\n",
    "3. ✅ Stopword removal helps focus on content words\n",
    "\n",
    "**Next step**: Move beyond lexical representations to **semantic embeddings** that capture meaning, not just word co-occurrence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de54d33",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Iteration 4 - Semantic Representations with Sentence Embeddings\n",
    "\n",
    "**What we're changing**: ONLY the data representation (TF-IDF → Sentence Embeddings)\n",
    "\n",
    "**What stays the same**: K-Means algorithm, number of clusters (k=5)\n",
    "\n",
    "**What are Sentence Embeddings?**\n",
    "\n",
    "**Sentence Embeddings** = Dense vector representations that capture semantic meaning\n",
    "\n",
    "**Key differences from TF-IDF**:\n",
    "- **Dense vs Sparse**: 384 dimensions (dense) vs 5000+ dimensions (sparse)\n",
    "- **Semantic vs Lexical**: Captures meaning vs counts words\n",
    "- **Word order matters**: \"dog bites man\" ≠ \"man bites dog\"\n",
    "- **Context-aware**: Same word, different meanings in different contexts\n",
    "- **Pre-trained**: Learned from massive text corpora (like the language model in Exercise 2)\n",
    "\n",
    "**The key insight**: Documents with similar meanings should have similar embeddings, even if they use different words!\n",
    "\n",
    "**Why K-Means (not LDA)?** LDA requires count data (multinomial distribution). Embeddings are continuous vectors, so K-Means is the standard choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c847217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sentence embeddings for all documents\n",
    "print(\"Generating sentence embeddings...\")\n",
    "print(f\"  Using model: {embedding_model}\")\n",
    "print(f\"  Processing {len(df)} documents...\")\n",
    "\n",
    "# Encode all documents (this may take a minute)\n",
    "# The model was already loaded in the setup section\n",
    "embeddings = embedding_model.encode(\n",
    "    df['text'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Embeddings generated!\")\n",
    "print(f\"  Shape: {embeddings.shape}\")\n",
    "print(f\"  Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"  Much smaller than TF-IDF: {embeddings.shape[1]} vs {X_tfidf.shape[1]} dimensions\")\n",
    "print(f\"  Dense representation: every dimension has a value (no zeros)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4c7652",
   "metadata": {},
   "source": [
    "### Visualizing Semantic Similarity\n",
    "\n",
    "Let's verify that embeddings capture semantic meaning by looking at similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bcab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find semantically similar documents\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Pick a sample document\n",
    "sample_idx = 0\n",
    "sample_text = df.iloc[sample_idx]['text']\n",
    "sample_category = df.iloc[sample_idx]['category']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SEMANTIC SIMILARITY DEMONSTRATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nSample document (Category: {sample_category}):\")\n",
    "print(f\"{sample_text[:300]}...\")\n",
    "\n",
    "# Calculate cosine similarity between sample and all other documents\n",
    "sample_embedding = embeddings[sample_idx].reshape(1, -1)\n",
    "similarities = cosine_similarity(sample_embedding, embeddings)[0]\n",
    "\n",
    "# Get top 5 most similar documents (excluding the sample itself)\n",
    "similar_indices = similarities.argsort()[::-1][1:6]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TOP 5 MOST SIMILAR DOCUMENTS:\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for rank, idx in enumerate(similar_indices, 1):\n",
    "    similarity = similarities[idx]\n",
    "    category = df.iloc[idx]['category']\n",
    "    text = df.iloc[idx]['text']\n",
    "    \n",
    "    print(f\"\\n{rank}. Similarity: {similarity:.3f} | Category: {category}\")\n",
    "    print(f\"   {text[:200]}...\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✓ OBSERVATION: Similar documents have high cosine similarity\")\n",
    "print(\"Notice how semantically related documents are found, even with different words!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aca3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embedding space in 2D using PCA\n",
    "print(\"Visualizing embedding space in 2D...\")\n",
    "\n",
    "# Reduce embeddings to 2D for visualization\n",
    "pca_emb = PCA(n_components=2, random_state=42)\n",
    "embeddings_2d = pca_emb.fit_transform(embeddings)\n",
    "\n",
    "print(f\"✓ PCA complete\")\n",
    "print(f\"  Variance explained: {100*pca_emb.explained_variance_ratio_.sum():.1f}%\")\n",
    "\n",
    "# Plot by category\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for category in df['category'].unique():\n",
    "    mask = df['category'] == category\n",
    "    plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n",
    "                label=category, alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "plt.xlabel('First Principal Component', fontsize=12)\n",
    "plt.ylabel('Second Principal Component', fontsize=12)\n",
    "plt.title('Document Embeddings in 2D Space\\nDocuments cluster by semantic similarity', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10, loc='best')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ OBSERVATION: Documents with similar meanings cluster together\")\n",
    "print(\"Even in 2D, we can see semantic structure!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a739e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means clustering on embeddings\n",
    "print(\"Applying K-Means clustering to embeddings...\")\n",
    "\n",
    "kmeans_emb = KMeans(\n",
    "    n_clusters=n_clusters,\n",
    "    random_state=42,\n",
    "    n_init=10,\n",
    "    max_iter=300\n",
    ")\n",
    "\n",
    "clusters_emb = kmeans_emb.fit_predict(embeddings)\n",
    "\n",
    "print(f\"✓ K-Means clustering complete\")\n",
    "print(f\"  Number of clusters: {n_clusters}\")\n",
    "print(f\"  Iterations to converge: {kmeans_emb.n_iter_}\")\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df['cluster_emb'] = clusters_emb\n",
    "\n",
    "# Show cluster sizes\n",
    "print(f\"\\n  Cluster sizes:\")\n",
    "for i in range(n_clusters):\n",
    "    count = (clusters_emb == i).sum()\n",
    "    print(f\"    Cluster {i}: {count} documents ({100*count/len(df):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e174a253",
   "metadata": {},
   "source": [
    "### Diagnostic 1: Top Words Per Cluster (Embeddings)\n",
    "\n",
    "Since embeddings don't directly give us words, we'll reuse TF-IDF to look at the most common words in documents assigned to each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ac1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words for embedding-based clusters\n",
    "# We'll use TF-IDF scores within each cluster to find characteristic words\n",
    "top_words_emb = get_top_words_per_cluster(X_tfidf, vectorizer_tfidf, clusters_emb, n_words=10)\n",
    "\n",
    "# Display\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP 10 WORDS PER CLUSTER (Embeddings + K-Means)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for cluster_id, words in top_words_emb.items():\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    word_list = [f\"{word} ({score:.3f})\" for word, score in words]\n",
    "    print(f\"  {', '.join(word_list)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ OBSERVATION: Clusters based on semantic meaning\")\n",
    "print(\"Words reflect the semantic themes, not just lexical co-occurrence\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d60918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top words for each cluster\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for cluster_id in range(n_clusters):\n",
    "    words, scores = zip(*top_words_emb[cluster_id])\n",
    "    \n",
    "    axes[cluster_id].barh(range(len(words)), scores, color='teal')\n",
    "    axes[cluster_id].set_yticks(range(len(words)))\n",
    "    axes[cluster_id].set_yticklabels(words)\n",
    "    axes[cluster_id].set_xlabel('Average TF-IDF Score', fontsize=10)\n",
    "    axes[cluster_id].set_title(f'Cluster {cluster_id} Top Words', fontsize=11, fontweight='bold')\n",
    "    axes[cluster_id].invert_yaxis()\n",
    "    axes[cluster_id].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.suptitle('Top Words Per Cluster (Embeddings + K-Means)', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e5d9c6",
   "metadata": {},
   "source": [
    "### Diagnostic 2: Cluster Quality Metrics (Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe79b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cluster quality metrics for embeddings\n",
    "print(\"Calculating cluster quality metrics for embeddings...\")\n",
    "\n",
    "# Cluster balance\n",
    "cluster_sizes_emb = np.bincount(clusters_emb)\n",
    "cluster_balance_emb = entropy(cluster_sizes_emb / cluster_sizes_emb.sum()) / np.log(n_clusters)\n",
    "\n",
    "# Cluster purity\n",
    "purity_emb = cluster_purity(df['category'], clusters_emb)\n",
    "\n",
    "# Top word distinctiveness\n",
    "word_overlap_emb = top_word_overlap(top_words_emb)\n",
    "\n",
    "print(f\"✓ Cluster quality analysis complete\")\n",
    "print(f\"\\n  Cluster balance: {cluster_balance_emb:.3f}\")\n",
    "print(f\"  Cluster purity: {purity_emb:.3f}\")\n",
    "print(f\"  Top word overlap: {word_overlap_emb:.3f}\")\n",
    "print(f\"\\n  Comparison:\")\n",
    "print(f\"    BoW:       balance={cluster_balance:.3f}, purity={purity_bow:.3f}\")\n",
    "print(f\"    TF-IDF:    balance={cluster_balance_tfidf:.3f}, purity={purity_tfidf:.3f}\")\n",
    "print(f\"    LDA:       balance={cluster_balance_lda:.3f}, purity={purity_lda:.3f}\")\n",
    "print(f\"    Embeddings: balance={cluster_balance_emb:.3f}, purity={purity_emb:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b010b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all methods including embeddings\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "# Row 1: Cluster size distributions\n",
    "methods = ['BoW\\n+K-Means', 'TF-IDF\\n+K-Means', 'BoW\\n+LDA', 'Embeddings\\n+K-Means']\n",
    "balances = [cluster_balance, cluster_balance_tfidf, cluster_balance_lda, cluster_balance_emb]\n",
    "size_dists = [\n",
    "    cluster_sizes / cluster_sizes.sum() * 100,\n",
    "    cluster_sizes_tfidf / cluster_sizes_tfidf.sum() * 100,\n",
    "    topic_sizes / topic_sizes.sum() * 100,\n",
    "    cluster_sizes_emb / cluster_sizes_emb.sum() * 100\n",
    "]\n",
    "colors_bar = ['steelblue', 'lightgreen', 'mediumpurple', 'teal']\n",
    "\n",
    "for i, (method, balance, sizes, color) in enumerate(zip(methods, balances, size_dists, colors_bar)):\n",
    "    axes[0, i].bar(range(n_clusters), sizes, color=color, edgecolor='black')\n",
    "    axes[0, i].axhline(y=100/n_clusters, color='red', linestyle='--', linewidth=2)\n",
    "    axes[0, i].set_xlabel('Cluster/Topic ID', fontsize=9)\n",
    "    axes[0, i].set_ylabel('Percentage', fontsize=9)\n",
    "    axes[0, i].set_title(f'{method}\\nBalance = {balance:.3f}', fontsize=10, fontweight='bold')\n",
    "    axes[0, i].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Row 2: Cluster purity (composition)\n",
    "purities = [purity_bow, purity_tfidf, purity_lda, purity_emb]\n",
    "cluster_labels_all = [clusters_bow, clusters_tfidf, dominant_topics, clusters_emb]\n",
    "contingencies = [\n",
    "    pd.crosstab(labels, df['category'], normalize='index') * 100\n",
    "    for labels in cluster_labels_all\n",
    "]\n",
    "\n",
    "for i, (method, purity, cont, color) in enumerate(zip(methods, purities, contingencies, colors_bar)):\n",
    "    cont.plot(kind='bar', stacked=True, ax=axes[1, i], colormap='Set3', legend=(i==3))\n",
    "    axes[1, i].set_xlabel('Cluster/Topic ID', fontsize=9)\n",
    "    axes[1, i].set_ylabel('Percentage', fontsize=9)\n",
    "    axes[1, i].set_title(f'{method}\\nPurity = {purity:.3f}', fontsize=10, fontweight='bold')\n",
    "    axes[1, i].set_xticklabels(axes[1, i].get_xticklabels(), rotation=0)\n",
    "    axes[1, i].grid(axis='y', alpha=0.3)\n",
    "    if i == 3:\n",
    "        axes[1, i].legend(title='Category', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "\n",
    "plt.suptitle('Progression: Lexical → Semantic Representations', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ IMPROVEMENT: Embeddings achieve even better purity and balance. We've almost constructued the original categories from topics alone!\")\n",
    "print(\"Semantic understanding helps group documents more meaningfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ecec35",
   "metadata": {},
   "source": [
    "### Diagnostic 3: 2D Visualization Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5af91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare TF-IDF vs Embeddings clustering in 2D\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: TF-IDF + K-Means\n",
    "for cluster_id in range(n_clusters):\n",
    "    mask = clusters_tfidf == cluster_id\n",
    "    ax1.scatter(X_tfidf_2d[mask, 0], X_tfidf_2d[mask, 1],\n",
    "                c=[colors[cluster_id]], label=f'Cluster {cluster_id}',\n",
    "                alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "ax1.set_xlabel('First Principal Component', fontsize=12)\n",
    "ax1.set_ylabel('Second Principal Component', fontsize=12)\n",
    "ax1.set_title(f'TF-IDF + K-Means (Purity: {purity_tfidf:.3f})', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=9, loc='best')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Right: Embeddings + K-Means\n",
    "for cluster_id in range(n_clusters):\n",
    "    mask = clusters_emb == cluster_id\n",
    "    ax2.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n",
    "                c=[colors[cluster_id]], label=f'Cluster {cluster_id}',\n",
    "                alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "ax2.set_xlabel('First Principal Component', fontsize=12)\n",
    "ax2.set_ylabel('Second Principal Component', fontsize=12)\n",
    "ax2.set_title(f'Embeddings + K-Means (Purity: {purity_emb:.3f})', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=9, loc='best')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Lexical vs Semantic Clustering', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ OBSERVATION: Embeddings create more semantically coherent clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aace35",
   "metadata": {},
   "source": [
    "### Reflection: What Did We Learn?\n",
    "\n",
    "**Improvements with Embeddings**:\n",
    "1. ✅ **Semantic understanding**: Captures meaning, not just word counts\n",
    "2. ✅ **Word order matters**: \"dog bites man\" ≠ \"man bites dog\"\n",
    "3. ✅ **Dense representation**: 384 dimensions vs 5000+ (more efficient)\n",
    "4. ✅ **Better clustering**: Higher purity, balance and low top-word overlap\n",
    "5. ✅ **Synonym handling**: Similar meanings → similar vectors\n",
    "\n",
    "**Trade-offs**:\n",
    "- ⚠️ **Computational cost**: Generating embeddings takes time\n",
    "- ⚠️ **Black box**: Harder to interpret than word counts\n",
    "- ⚠️ **Model dependency**: Quality depends on pre-training data\n",
    "\n",
    "**When to use embeddings**:\n",
    "- ✅ Semantic similarity is important\n",
    "- ✅ Documents use varied vocabulary\n",
    "- ✅ You have computational resources\n",
    "- ✅ You want state-of-the-art performance\n",
    "\n",
    "**Next step**: Combine embeddings with density-based clustering (HDBSCAN) to automatically discover the number of topics!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8692caf3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## EXTRA CREDIT: Iteration 5 - Automatic Cluster Discovery with HDBSCAN\n",
    "\n",
    "**What we're changing**: Algorithm (K-Means → HDBSCAN) + addressing dimensionality\n",
    "\n",
    "**What stays the same**: Semantic embeddings representation\n",
    "\n",
    "**The problem with k=5**: We've been assuming 5 topics because the dataset has 5 categories. But:\n",
    "- Are news categories the same as topics?\n",
    "- Topics might be more granular (e.g., \"elections\", \"stock markets\", \"football transfers\")\n",
    "- Or more abstract (e.g., \"conflict\", \"innovation\", \"regulation\")\n",
    "\n",
    "**What is HDBSCAN?**\n",
    "\n",
    "**HDBSCAN** = Hierarchical Density-Based Spatial Clustering of Applications with Noise\n",
    "\n",
    "**Key features**:\n",
    "- **No need to specify k**: Discovers clusters based on data density\n",
    "- **Handles noise**: Can label outliers (documents that don't fit any cluster)\n",
    "- **Arbitrary shapes**: Finds clusters of any shape, not just spherical\n",
    "- **Hierarchical**: Builds a hierarchy of clusters at different scales\n",
    "\n",
    "**The challenge**: Even 384 dimensions (embeddings) can suffer from the curse of dimensionality for density-based methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f8969",
   "metadata": {},
   "source": [
    "### Initial Attempt: HDBSCAN on Raw Embeddings\n",
    "\n",
    "Let's first try HDBSCAN directly on our 384-dimensional embeddings to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e305e48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply HDBSCAN to raw embeddings\n",
    "print(\"Applying HDBSCAN to raw embeddings (384 dimensions)...\")\n",
    "\n",
    "clusterer_hdbscan_initial = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=30,\n",
    "    min_samples=5,\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom'\n",
    ")\n",
    "\n",
    "clusters_hdbscan_initial = clusterer_hdbscan_initial.fit_predict(embeddings)\n",
    "\n",
    "# Count clusters\n",
    "n_clusters_initial = len(set(clusters_hdbscan_initial)) - (1 if -1 in clusters_hdbscan_initial else 0)\n",
    "n_noise_initial = (clusters_hdbscan_initial == -1).sum()\n",
    "\n",
    "print(f\"✓ HDBSCAN clustering complete\")\n",
    "print(f\"  Clusters found: {n_clusters_initial}\")\n",
    "print(f\"  Noise points: {n_noise_initial} ({100*n_noise_initial/len(df):.1f}%)\")\n",
    "\n",
    "if n_noise_initial / len(df) > 0.3:\n",
    "    print(f\"\\n  ⚠️ WARNING: High percentage of noise!\")\n",
    "    print(f\"  Even 384 dimensions can be too high for density-based clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f477e4d0",
   "metadata": {},
   "source": [
    "### The Curse of Dimensionality\n",
    "\n",
    "Even though embeddings are much lower-dimensional than TF-IDF (384 vs 5000), they can still suffer from the curse of dimensionality for density-based methods.\n",
    "\n",
    "**The problem**: In high-dimensional spaces, distances become less meaningful:\n",
    "- All points appear roughly equidistant\n",
    "- \"Dense\" regions are actually sparse\n",
    "- HDBSCAN struggles to find meaningful clusters\n",
    "\n",
    "**The solution**: **UMAP** (Uniform Manifold Approximation and Projection)\n",
    "- Non-linear dimensionality reduction\n",
    "- Preserves both local and global structure\n",
    "- Designed specifically for clustering (unlike PCA which focuses on variance)\n",
    "- Reduces to ~50 dimensions while maintaining semantic relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e94275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the curse of dimensionality with embeddings\n",
    "print(\"Demonstrating curse of dimensionality in embedding space...\")\n",
    "\n",
    "# Sample documents for faster computation\n",
    "sample_size = 500\n",
    "sample_indices_emb = np.random.choice(len(embeddings), size=sample_size, replace=False)\n",
    "embeddings_sample = embeddings[sample_indices_emb]\n",
    "\n",
    "# Calculate pairwise distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "distances_emb = euclidean_distances(embeddings_sample)\n",
    "\n",
    "# Get distances to nearest neighbor\n",
    "nearest_distances_emb = np.partition(distances_emb, 1, axis=1)[:, 1]\n",
    "\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(nearest_distances_emb, bins=50, color='teal', edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Distance to Nearest Neighbor', fontsize=12)\n",
    "plt.ylabel('Number of Documents', fontsize=12)\n",
    "plt.title(f'Embedding Space ({embeddings.shape[1]} dimensions)\\nStill high-dimensional!', \n",
    "          fontsize=12, fontweight='bold')\n",
    "plt.axvline(x=nearest_distances_emb.mean(), color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Mean = {nearest_distances_emb.mean():.2f}')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "all_distances_emb = distances_emb[np.triu_indices_from(distances_emb, k=1)]\n",
    "plt.hist(all_distances_emb, bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Pairwise Distance', fontsize=12)\n",
    "plt.ylabel('Number of Pairs', fontsize=12)\n",
    "plt.title('Distribution of All Pairwise Distances\\nNarrow range = curse of dimensionality', \n",
    "          fontsize=12, fontweight='bold')\n",
    "plt.axvline(x=all_distances_emb.mean(), color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Mean = {all_distances_emb.mean():.2f}')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "ratio_emb = nearest_distances_emb.std() / nearest_distances_emb.mean()\n",
    "print(f\"\\n✓ OBSERVATION: In {embeddings.shape[1]} dimensions:\")\n",
    "print(f\"  Distance variability (std/mean): {ratio_emb:.3f}\")\n",
    "print(f\"  Still relatively low → distances are less meaningful because everything is about equally close\")\n",
    "print(f\"  Solution: UMAP to reduce to ~50 dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a908d4",
   "metadata": {},
   "source": [
    "### Solution: UMAP Dimensionality Reduction\n",
    "\n",
    "We'll use **UMAP** to reduce from 384 dimensions to 50 dimensions.\n",
    "\n",
    "**Why UMAP (not PCA)?**\n",
    "- **Non-linear**: Captures complex relationships PCA misses\n",
    "- **Preserves local structure**: Keeps similar documents close together\n",
    "- **Preserves global structure**: Maintains overall data topology\n",
    "- **Designed for clustering**: Works better with density-based methods\n",
    "\n",
    "**Why 50 dimensions?**\n",
    "- Balances information retention and curse of dimensionality\n",
    "- Enough to preserve semantic structure\n",
    "- Low enough for HDBSCAN to work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a9839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply UMAP for dimensionality reduction\n",
    "print(\"Applying UMAP dimensionality reduction...\")\n",
    "print(f\"  Reducing from {embeddings.shape[1]} to 50 dimensions...\")\n",
    "\n",
    "# UMAP parameters\n",
    "n_components_umap = 50\n",
    "n_neighbors = 15  # Balance between local and global structure\n",
    "min_dist = 0.1    # Minimum distance between points in low-dimensional space\n",
    "\n",
    "umap_reducer = umap.UMAP(\n",
    "    n_components=n_components_umap,\n",
    "    n_neighbors=n_neighbors,\n",
    "    min_dist=min_dist,\n",
    "    metric='cosine',  # Good for embeddings\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Fit and transform (this may take a minute)\n",
    "embeddings_umap = umap_reducer.fit_transform(embeddings)\n",
    "\n",
    "print(f\"\\n✓ UMAP reduction complete\")\n",
    "print(f\"  Original shape: {embeddings.shape}\")\n",
    "print(f\"  Reduced shape: {embeddings_umap.shape}\")\n",
    "print(f\"  Compression ratio: {embeddings.shape[1] / embeddings_umap.shape[1]:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910efad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that distances are more meaningful after UMAP\n",
    "print(\"Comparing distances before and after UMAP...\")\n",
    "\n",
    "# Sample for faster computation\n",
    "embeddings_umap_sample = embeddings_umap[sample_indices_emb]\n",
    "\n",
    "# Calculate distances in UMAP space\n",
    "distances_umap = euclidean_distances(embeddings_umap_sample)\n",
    "nearest_distances_umap = np.partition(distances_umap, 1, axis=1)[:, 1]\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(nearest_distances_emb, bins=50, color='teal', edgecolor='black', alpha=0.7, label='Original (384D)')\n",
    "plt.hist(nearest_distances_umap, bins=50, color='orange', edgecolor='black', alpha=0.7, label='After UMAP (50D)')\n",
    "plt.xlabel('Distance to Nearest Neighbor', fontsize=12)\n",
    "plt.ylabel('Number of Documents', fontsize=12)\n",
    "plt.title('Nearest Neighbor Distances: Before vs After UMAP', fontsize=12, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Compare ratios\n",
    "ratio_umap = nearest_distances_umap.std() / nearest_distances_umap.mean()\n",
    "\n",
    "plt.bar(['Original\\n(384D)', f'UMAP\\n({n_components_umap}D)'], \n",
    "        [ratio_emb, ratio_umap],\n",
    "        color=['teal', 'orange'], edgecolor='black')\n",
    "plt.ylabel('Std/Mean Ratio', fontsize=12)\n",
    "plt.title('Distance Variability (Higher = Better Separation)', fontsize=12, fontweight='bold')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ IMPROVEMENT:\")\n",
    "print(f\"  Original space: std/mean = {ratio_emb:.3f}\")\n",
    "print(f\"  UMAP space: std/mean = {ratio_umap:.3f}\")\n",
    "print(f\"  Change: {((ratio_umap - ratio_emb) / ratio_emb * 100):+.1f}%\")\n",
    "print(f\"\\n  Higher ratio = better separation between near and far points\")\n",
    "print(f\"  UMAP makes distances more meaningful for HDBSCAN!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287caf79",
   "metadata": {},
   "source": [
    "### Re-running HDBSCAN on UMAP-Reduced Embeddings\n",
    "\n",
    "Now let's apply HDBSCAN to the UMAP-reduced embeddings and see if we get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05159a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply HDBSCAN to UMAP-reduced embeddings\n",
    "print(\"Applying HDBSCAN to UMAP-reduced embeddings...\")\n",
    "\n",
    "clusterer_hdbscan = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=30,\n",
    "    min_samples=5,\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom'\n",
    ")\n",
    "\n",
    "clusters_hdbscan = clusterer_hdbscan.fit_predict(embeddings_umap)\n",
    "\n",
    "print(f\"✓ HDBSCAN clustering complete\")\n",
    "\n",
    "# Count clusters\n",
    "n_clusters_found = len(set(clusters_hdbscan)) - (1 if -1 in clusters_hdbscan else 0)\n",
    "n_noise = (clusters_hdbscan == -1).sum()\n",
    "\n",
    "print(f\"  Clusters found: {n_clusters_found}\")\n",
    "print(f\"  Noise points: {n_noise} ({100*n_noise/len(df):.1f}%)\")\n",
    "\n",
    "# Compare with initial attempt\n",
    "print(f\"\\n  Comparison with high-dimensional clustering:\")\n",
    "print(f\"    Before UMAP: {n_clusters_initial} clusters, {100*n_noise_initial/len(df):.1f}% noise\")\n",
    "print(f\"    After UMAP:  {n_clusters_found} clusters, {100*n_noise/len(df):.1f}% noise\")\n",
    "print(f\"    Improvement: {n_noise_initial - n_noise} fewer noise points!\")\n",
    "\n",
    "# Add to dataframe\n",
    "df['cluster_hdbscan'] = clusters_hdbscan\n",
    "\n",
    "# Show cluster sizes\n",
    "print(f\"\\n  Cluster sizes:\")\n",
    "for i in sorted(set(clusters_hdbscan)):\n",
    "    if i == -1:\n",
    "        print(f\"    Noise: {(clusters_hdbscan == i).sum()} documents ({100*(clusters_hdbscan == i).sum()/len(df):.1f}%)\")\n",
    "    else:\n",
    "        count = (clusters_hdbscan == i).sum()\n",
    "        print(f\"    Cluster {i}: {count} documents ({100*count/len(df):.1f}%)\")\n",
    "\n",
    "if n_noise / len(df) < 0.3:\n",
    "    print(f\"\\n  ✅ SUCCESS: Noise reduced to acceptable level!\")\n",
    "else:\n",
    "    print(f\"\\n  ⚠️ Still high noise - may need to adjust parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d89e2b",
   "metadata": {},
   "source": [
    "### Diagnostic 1: Top Words Per Cluster (HDBSCAN)\n",
    "\n",
    "Let's see what topics HDBSCAN discovered without being told how many to find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218e34ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top words for HDBSCAN clusters (excluding noise)\n",
    "valid_clusters = [c for c in set(clusters_hdbscan) if c != -1]\n",
    "top_words_hdbscan = {}\n",
    "\n",
    "for cluster_id in valid_clusters:\n",
    "    cluster_mask = clusters_hdbscan == cluster_id\n",
    "    cluster_docs = X_tfidf[cluster_mask]\n",
    "    \n",
    "    # Average TF-IDF scores across cluster\n",
    "    avg_scores = np.asarray(cluster_docs.mean(axis=0)).flatten()\n",
    "    \n",
    "    # Get top words\n",
    "    feature_names = vectorizer_tfidf.get_feature_names_out()\n",
    "    top_indices = avg_scores.argsort()[-10:][::-1]\n",
    "    top_words_hdbscan[cluster_id] = [(feature_names[i], avg_scores[i]) for i in top_indices]\n",
    "\n",
    "# Display\n",
    "print(\"=\" * 80)\n",
    "print(f\"TOP 10 WORDS PER CLUSTER (HDBSCAN - {n_clusters_found} clusters discovered)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for cluster_id in sorted(valid_clusters):\n",
    "    words = top_words_hdbscan[cluster_id]\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    word_list = [f\"{word} ({score:.3f})\" for word, score in words]\n",
    "    print(f\"  {', '.join(word_list)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"✓ OBSERVATION: HDBSCAN found {n_clusters_found} clusters (not 5!)\")\n",
    "print(\"This suggests topics might be more granular than news categories.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a66d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top words for each HDBSCAN cluster\n",
    "n_cols = 3\n",
    "n_rows = (n_clusters_found + n_cols - 1) // n_cols  # Ceiling division\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 5*n_rows))\n",
    "if n_clusters_found > 1:\n",
    "    axes = axes.flatten()\n",
    "else:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, cluster_id in enumerate(sorted(valid_clusters)):\n",
    "    words, scores = zip(*top_words_hdbscan[cluster_id])\n",
    "    \n",
    "    axes[idx].barh(range(len(words)), scores, color='coral')\n",
    "    axes[idx].set_yticks(range(len(words)))\n",
    "    axes[idx].set_yticklabels(words)\n",
    "    axes[idx].set_xlabel('Average TF-IDF Score', fontsize=10)\n",
    "    axes[idx].set_title(f'Cluster {cluster_id} Top Words', fontsize=11, fontweight='bold')\n",
    "    axes[idx].invert_yaxis()\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Remove extra subplots\n",
    "for idx in range(n_clusters_found, len(axes)):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.suptitle(f'Top Words Per Cluster (HDBSCAN) - {n_clusters_found} Clusters Discovered', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bd7c30",
   "metadata": {},
   "source": [
    "### Diagnostic 2: Cluster Quality Metrics (HDBSCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9484f397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cluster quality metrics for HDBSCAN\n",
    "print(\"Calculating cluster quality metrics for HDBSCAN...\")\n",
    "\n",
    "# For metrics, we'll exclude noise points (-1)\n",
    "non_noise_mask = clusters_hdbscan != -1\n",
    "clusters_hdbscan_no_noise = clusters_hdbscan[non_noise_mask]\n",
    "categories_no_noise = df['category'][non_noise_mask]\n",
    "\n",
    "# Cluster balance (excluding noise)\n",
    "cluster_sizes_hdbscan = np.bincount(clusters_hdbscan_no_noise)\n",
    "cluster_balance_hdbscan = entropy(cluster_sizes_hdbscan / cluster_sizes_hdbscan.sum()) / np.log(len(cluster_sizes_hdbscan))\n",
    "\n",
    "# Cluster purity (excluding noise)\n",
    "purity_hdbscan = cluster_purity(categories_no_noise, clusters_hdbscan_no_noise)\n",
    "\n",
    "# Top word distinctiveness\n",
    "word_overlap_hdbscan = top_word_overlap(top_words_hdbscan)\n",
    "\n",
    "print(f\"✓ Cluster quality analysis complete\")\n",
    "print(f\"\\n  Clusters found: {n_clusters_found}\")\n",
    "print(f\"  Noise points: {n_noise} ({100*n_noise/len(df):.1f}%)\")\n",
    "print(f\"  Cluster balance: {cluster_balance_hdbscan:.3f}\")\n",
    "print(f\"  Cluster purity: {purity_hdbscan:.3f}\")\n",
    "print(f\"  Top word overlap: {word_overlap_hdbscan:.3f}\")\n",
    "print(f\"\\n  Comparison with k=5 methods:\")\n",
    "print(f\"    BoW:        balance={cluster_balance:.3f}, purity={purity_bow:.3f}\")\n",
    "print(f\"    TF-IDF:     balance={cluster_balance_tfidf:.3f}, purity={purity_tfidf:.3f}\")\n",
    "print(f\"    LDA:        balance={cluster_balance_lda:.3f}, purity={purity_lda:.3f}\")\n",
    "print(f\"    Embeddings: balance={cluster_balance_emb:.3f}, purity={purity_emb:.3f}\")\n",
    "print(f\"    HDBSCAN:    balance={cluster_balance_hdbscan:.3f}, purity={purity_hdbscan:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be8f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster sizes and composition\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Cluster size distribution\n",
    "cluster_sizes_hdbscan_pct = cluster_sizes_hdbscan / cluster_sizes_hdbscan.sum() * 100\n",
    "axes[0].bar(range(len(cluster_sizes_hdbscan)), cluster_sizes_hdbscan_pct, color='coral', edgecolor='black')\n",
    "axes[0].set_xlabel('Cluster ID', fontsize=12)\n",
    "axes[0].set_ylabel('Percentage of Documents (excl. noise)', fontsize=12)\n",
    "axes[0].set_title(f'HDBSCAN: {n_clusters_found} Clusters (Balance = {cluster_balance_hdbscan:.3f})', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Right: Cluster composition by category (excluding noise)\n",
    "contingency_hdbscan = pd.crosstab(clusters_hdbscan_no_noise, categories_no_noise, normalize='index') * 100\n",
    "contingency_hdbscan.plot(kind='bar', stacked=True, ax=axes[1], colormap='Set3')\n",
    "axes[1].set_xlabel('Cluster ID', fontsize=12)\n",
    "axes[1].set_ylabel('Percentage', fontsize=12)\n",
    "axes[1].set_title(f'HDBSCAN: Composition (Purity = {purity_hdbscan:.3f})', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].legend(title='Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=0)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ OBSERVATION: HDBSCAN found {n_clusters_found} clusters instead of 5\")\n",
    "print(\"This suggests the data has more natural groupings than the 5 predefined categories!\")\n",
    "print(\"Purity and cluster balance have declined - primarily because business and politics have become difficult to separate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2b2abf",
   "metadata": {},
   "source": [
    "### Diagnostic 3: Analyzing Noise Points\n",
    "\n",
    "One unique feature of HDBSCAN is that it can identify outliers - documents that don't fit well into any cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7a3573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze noise points\n",
    "if n_noise > 0:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"NOISE POINT ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    noise_mask = clusters_hdbscan == -1\n",
    "    noise_docs = df[noise_mask]\n",
    "    \n",
    "    # Category distribution of noise\n",
    "    print(f\"\\nNoise points by category:\")\n",
    "    noise_categories = noise_docs['category'].value_counts()\n",
    "    for cat, count in noise_categories.items():\n",
    "        print(f\"  {cat}: {count} ({100*count/n_noise:.1f}% of noise)\")\n",
    "    \n",
    "    # Show a few examples\n",
    "    print(f\"\\nExample noise documents (don't fit well into any cluster):\")\n",
    "    for i, idx in enumerate(noise_docs.index[10:13]):\n",
    "        print(f\"\\n{'-'*80}\")\n",
    "        print(f\"Example {i+1}: Category = {df.iloc[idx]['category']}\")\n",
    "        print(f\"{df.iloc[idx]['text'][:800]}...\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"✓ INSIGHT: Noise points might be:\")\n",
    "    print(\"  - Documents that span multiple topics\")\n",
    "    print(\"  - Unusual or unique documents\")\n",
    "    print(\"  - Documents that don't fit the main themes\")\n",
    "else:\n",
    "    print(\"No noise points detected by HDBSCAN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3804964",
   "metadata": {},
   "source": [
    "### Diagnostic 4: Cluster Hierarchy and Confidence\n",
    "\n",
    "HDBSCAN builds a hierarchy of clusters and provides confidence scores for each assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ad1677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster hierarchy\n",
    "print(\"Visualizing cluster hierarchy...\")\n",
    "\n",
    "# Get cluster probabilities (how confident is HDBSCAN about each assignment?)\n",
    "cluster_probs = clusterer_hdbscan.probabilities_\n",
    "\n",
    "# Show distribution of cluster membership probabilities\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.hist(cluster_probs[non_noise_mask], bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
    "plt.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Low confidence threshold')\n",
    "plt.xlabel('Cluster Membership Probability', fontsize=12)\n",
    "plt.ylabel('Number of Documents', fontsize=12)\n",
    "plt.title('HDBSCAN: Cluster Membership Confidence\\n(Higher = More Confident Assignment)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "low_confidence = (cluster_probs < 0.5).sum()\n",
    "print(f\"\\nDocuments with low confidence (< 0.5): {low_confidence} ({100*low_confidence/len(df):.1f}%)\")\n",
    "print(\"These documents are on the boundary between clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80db325",
   "metadata": {},
   "source": [
    "### Diagnostic 5: Visualizing Clusters in 2D\n",
    "\n",
    "Let's visualize the HDBSCAN clusters in 2D space using UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce UMAP embeddings to 2D for visualization\n",
    "print(\"Reducing to 2D for visualization...\")\n",
    "\n",
    "umap_2d = umap.UMAP(\n",
    "    n_components=2,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.1,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "embeddings_2d_umap = umap_2d.fit_transform(embeddings)\n",
    "\n",
    "print(\"✓ 2D UMAP complete\")\n",
    "\n",
    "# Plot HDBSCAN clusters\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot each cluster with different color\n",
    "hdbscan_colors = plt.cm.tab20(np.linspace(0, 1, n_clusters_found + 1))\n",
    "\n",
    "for idx, cluster_id in enumerate(sorted(valid_clusters)):\n",
    "    mask = clusters_hdbscan == cluster_id\n",
    "    plt.scatter(embeddings_2d_umap[mask, 0], embeddings_2d_umap[mask, 1],\n",
    "                c=[hdbscan_colors[idx]], label=f'Cluster {cluster_id}',\n",
    "                alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "# Plot noise points\n",
    "if n_noise > 0:\n",
    "    noise_mask = clusters_hdbscan == -1\n",
    "    plt.scatter(embeddings_2d_umap[noise_mask, 0], embeddings_2d_umap[noise_mask, 1],\n",
    "                c='lightgray', label='Noise', marker='x',\n",
    "                alpha=0.5, s=30)\n",
    "\n",
    "plt.xlabel('UMAP Dimension 1', fontsize=12)\n",
    "plt.ylabel('UMAP Dimension 2', fontsize=12)\n",
    "plt.title(f'HDBSCAN Clusters in 2D (Embeddings + UMAP)\\n{n_clusters_found} Clusters Discovered', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=9, loc='best', ncol=2)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ OBSERVATION: HDBSCAN adapts to data density\")\n",
    "print(f\"It found {n_clusters_found} clusters instead of forcing k=5\")\n",
    "print(\"Note that a small 'bridge' of documents seem to connect the huge 'business' and 'politics' clusters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d7316b",
   "metadata": {},
   "source": [
    "### Reflection: What Did We Learn?\n",
    "\n",
    "**Key insights from HDBSCAN + Embeddings + UMAP**:\n",
    "\n",
    "1. ✅ **No need to specify k**: Found {n_clusters_found} clusters automatically\n",
    "2. ✅ **Semantic understanding**: Embeddings capture meaning\n",
    "3. ✅ **UMAP helps**: Reduces dimensionality while preserving structure\n",
    "4. ✅ **Identifies outliers**: {n_noise} documents labeled as noise\n",
    "5. ✅ **Confidence scores**: Shows which assignments are uncertain\n",
    "6. ✅ **Adapts to density**: Clusters can have different sizes naturally\n",
    "\n",
    "**The complete pipeline**:\n",
    "1. **Text → Embeddings**: Capture semantic meaning (384D)\n",
    "2. **Embeddings → UMAP**: Reduce dimensionality (50D)\n",
    "3. **UMAP → HDBSCAN**: Discover clusters automatically\n",
    "\n",
    "**When to use this approach**:\n",
    "- ✅ Exploratory analysis (don't know how many topics)\n",
    "- ✅ Semantic similarity is important\n",
    "- ✅ Uneven topic distributions\n",
    "- ✅ Want to identify outliers\n",
    "- ✅ Have computational resources\n",
    "\n",
    "**Trade-offs**:\n",
    "- ⚠️ **Computational cost**: Embeddings + UMAP + HDBSCAN takes time\n",
    "- ⚠️ **Complexity**: More moving parts than simple K-Means\n",
    "- ⚠️ **Interpretability**: Harder to explain than word counts\n",
    "- ⚠️ **Parameter sensitivity**: HDBSCAN results depend on min_cluster_size, min_samples\n",
    "\n",
    "**The big question**: Are news categories the same as topics?\n",
    "- HDBSCAN suggests: **No!** The data contains multiple separate dense regions.\n",
    "- This could mean topics are more granular, or categories overlap\n",
    "- Always question your assumptions about k!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34d22ec",
   "metadata": {},
   "source": [
    "### Final Comparison: All Methods\n",
    "\n",
    "Let's compare all the methods we've explored in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6e6c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "comparison_data = {\n",
    "    'Method': [\n",
    "        'BoW + K-Means',\n",
    "        'TF-IDF + K-Means',\n",
    "        'BoW + LDA',\n",
    "        'Embeddings + K-Means',\n",
    "        'Embeddings + UMAP + HDBSCAN'\n",
    "    ],\n",
    "    'Representation': [\n",
    "        'Lexical (sparse)',\n",
    "        'Lexical (sparse)',\n",
    "        'Lexical (sparse)',\n",
    "        'Semantic (dense)',\n",
    "        'Semantic (dense)'\n",
    "    ],\n",
    "    'Dimensions': [\n",
    "        X_bow.shape[1],\n",
    "        X_tfidf.shape[1],\n",
    "        X_lda.shape[1],\n",
    "        embeddings.shape[1],\n",
    "        embeddings_umap.shape[1]\n",
    "    ],\n",
    "    'Algorithm': [\n",
    "        'K-Means',\n",
    "        'K-Means',\n",
    "        'LDA',\n",
    "        'K-Means',\n",
    "        'HDBSCAN'\n",
    "    ],\n",
    "    'Clusters': [\n",
    "        n_clusters,\n",
    "        n_clusters,\n",
    "        n_topics,\n",
    "        n_clusters,\n",
    "        n_clusters_found\n",
    "    ],\n",
    "    'Balance': [\n",
    "        f'{cluster_balance:.3f}',\n",
    "        f'{cluster_balance_tfidf:.3f}',\n",
    "        f'{cluster_balance_lda:.3f}',\n",
    "        f'{cluster_balance_emb:.3f}',\n",
    "        f'{cluster_balance_hdbscan:.3f}'\n",
    "    ],\n",
    "    'Purity': [\n",
    "        f'{purity_bow:.3f}',\n",
    "        f'{purity_tfidf:.3f}',\n",
    "        f'{purity_lda:.3f}',\n",
    "        f'{purity_emb:.3f}',\n",
    "        f'{purity_hdbscan:.3f}'\n",
    "    ],\n",
    "    'Word Overlap': [\n",
    "        f'{word_overlap_bow:.3f}',\n",
    "        f'{word_overlap_tfidf:.3f}',\n",
    "        f'{word_overlap_lda:.3f}',\n",
    "        f'{word_overlap_emb:.3f}',\n",
    "        f'{word_overlap_hdbscan:.3f}'\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"COMPREHENSIVE METHOD COMPARISON\")\n",
    "print(\"=\" * 100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 100)\n",
    "print(\"\\n✓ Key Takeaways:\")\n",
    "print(\"  1. TF-IDF improves over BoW by downweighting common words\")\n",
    "print(\"  2. LDA provides soft clustering (documents can have multiple topics)\")\n",
    "print(\"  3. Embeddings capture semantic meaning (best purity)\")\n",
    "print(\"  4. HDBSCAN discovers natural number of clusters\")\n",
    "print(\"  5. UMAP helps HDBSCAN work in high-dimensional spaces\")\n",
    "print(\"\\n💡 Recommendation: Start simple (TF-IDF + K-Means), then try embeddings if needed\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fa2405",
   "metadata": {},
   "source": [
    "### Further Exploration\n",
    "\n",
    "If you want to go deeper:\n",
    "\n",
    "1. **Try different datasets**: Load your own data or try other public datasets\n",
    "2. **Experiment with parameters**: \n",
    "   - Different numbers of topics/clusters\n",
    "   - Different embedding models (all-mpnet-base-v2 for better quality)\n",
    "   - Different UMAP parameters (n_neighbors, min_dist)\n",
    "   - Different HDBSCAN parameters (min_cluster_size, min_samples)\n",
    "\n",
    "3. **Combine methods**: \n",
    "   - Use LDA to discover topics, then embeddings to find similar documents\n",
    "   - Use HDBSCAN to find number of clusters, then K-Means with that k\n",
    "   - Use embeddings for similarity, TF-IDF for interpretation\n",
    "\n",
    "4. **Advanced techniques**:\n",
    "   - **BERTopic**: Combines embeddings, UMAP, and HDBSCAN automatically\n",
    "   - **Top2Vec**: Similar to BERTopic but with different approach\n",
    "   - **Fine-tuning**: Train embeddings on your specific domain\n",
    "   - **Hierarchical clustering**: Explore topic hierarchies\n",
    "\n",
    "5. **Temporal analysis**: If your data has timestamps, track topic evolution over time\n",
    "\n",
    "6. **Multilingual analysis**: Modern embedding models work across languages!\n",
    "\n",
    "7. **Interactive exploration**: Use tools like Jupyter widgets to explore clusters interactively\n",
    "\n",
    "**Resources**:\n",
    "- **Sentence Transformers**: https://www.sbert.net/\n",
    "- **UMAP**: https://umap-learn.readthedocs.io/\n",
    "- **HDBSCAN**: https://hdbscan.readthedocs.io/\n",
    "- **BERTopic**: https://maartengr.github.io/BERTopic/\n",
    "- **Scikit-learn**: https://scikit-learn.org/stable/modules/clustering.html\n",
    "- **Hugging Face**: https://huggingface.co/models (for more embedding models)\n",
    "\n",
    "**Papers**:\n",
    "- Blei et al. (2003): \"Latent Dirichlet Allocation\" (LDA)\n",
    "- McInnes et al. (2018): \"UMAP: Uniform Manifold Approximation and Projection\"\n",
    "- McInnes et al. (2017): \"hdbscan: Hierarchical density based clustering\"\n",
    "- Reimers & Gurevych (2019): \"Sentence-BERT\" (sentence embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e07db0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introduction-to-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
