{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4584017",
   "metadata": {},
   "source": [
    "# Exercise 2 — Character-Level Language Modeling\n",
    "\n",
    "## Why Language Modeling?\n",
    "\n",
    "With over 700 million weekly active users in just a few years, ChatGPT is now the fasted adopted technology in history. At the core of ChatGPT and practically all modern AI tools are *Large Language Models (LLMs)* that enable rich functionality in engaging with human languages both in terms of generation and understanding.\n",
    "\n",
    "At their core, practically all modern language models, from the simplest to GPT-4, are ML models trained to **predict the next token** (word or character) given previous tokens. This seemingly simple task is the foundation of all modern AI language capabilities.\n",
    "\n",
    "In this exercise, we'll build language models from scratch to understand:\n",
    "- **How ML \"learns\" language** through statistical patterns\n",
    "- **Why design choices matter** (data representation, model architecture, inductive biases)\n",
    "- **Where modern LLMs come from** and what makes them powerful\n",
    "- **The fundamental principles** that scale from our tiny models to billion-parameter systems\n",
    "\n",
    "By the end, you'll have built two language models and understand the core ideas behind ChatGPT, just at a much smaller scale!\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this exercise, you will:\n",
    "1. Build two different language models from scratch using PyTorch\n",
    "2. Understand how **data representation** choices affect model performance\n",
    "3. Explore how **inductive biases** (assumptions about the problem) shape what models can learn\n",
    "4. See that machine learning is fundamentally a **modeling science** — our design choices matter!\n",
    "\n",
    "We'll work with Shakespeare's text and build increasingly sophisticated models, comparing their strengths and limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f5c841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and deterministic seed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Stable RNG for reproducibility\n",
    "g = torch.Generator().manual_seed(67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eb5b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the tiny Shakespeare dataset\n",
    "data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "response = requests.get(data_url)\n",
    "response.raise_for_status()\n",
    "text = response.text\n",
    "\n",
    "print(f\"Total length of the dataset (characters): {len(text):,}\")\n",
    "print('-' * 30)\n",
    "print('A short sample of the text:')\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4896bd",
   "metadata": {},
   "source": [
    "## Vocabulary & Tokenization\n",
    "\n",
    "**Design Choice #1: Character-level vs Word-level**\n",
    "\n",
    "We'll use **character-level** tokenization, meaning each character is a token. This is simpler than word-level (which would require handling ~30,000 words) and allows the model to generate any word, even ones not in the training data.\n",
    "\n",
    "**Modern LLMs** typically use subword tokenization (like BPE or WordPiece), which is a middle ground between characters and words (think 'swim', 'ing', and '<p>' being separate tokens). But the principles are the same!\n",
    "\n",
    "Trade-off: Character-level models need to learn spelling and word formation, making the task harder.\n",
    "\n",
    "**Why Cross-Entropy Loss?** For classification tasks (predicting which character comes next), cross-entropy measures how well our predicted probability distribution matches the true distribution. It heavily penalizes confident wrong predictions. This is the same loss function used in GPT models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13bca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary (characters) and mappings\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "# Reserve index 0 for a special token\n",
    "special_token = '<pad>'\n",
    "\n",
    "# stoi: string -> int, start from 1\n",
    "stoi = {ch: i + 1 for i, ch in enumerate(chars)}\n",
    "stoi[special_token] = 0\n",
    "\n",
    "# itos: int -> string\n",
    "itos = {i + 1: ch for i, ch in enumerate(chars)}\n",
    "itos[0] = special_token\n",
    "\n",
    "vocab_size = len(chars) + 1  # +1 for the special token\n",
    "print(f\"Vocabulary size (including '{special_token}'): {vocab_size}\")\n",
    "print('Characters:', ''.join(chars))\n",
    "\n",
    "# Encoder and decoder convenience functions\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join(itos[i] for i in l)\n",
    "\n",
    "# Quick sanity check\n",
    "sample_text = \"to be or not to be\"\n",
    "encoded_sample = encode(sample_text)\n",
    "decoded_sample = decode(encoded_sample)\n",
    "\n",
    "print('\\nSample Text:', sample_text)\n",
    "print('Encoded:', encoded_sample)\n",
    "print('Decoded:', decoded_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1e0888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert entire text to a single tensor and split into train/validation\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"Data tensor shape: {data.shape}\")\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"Train data: {len(train_data):,} characters\")\n",
    "print(f\"Validation data: {len(val_data):,} characters\")\n",
    "print(f\"\\nTrain sample: \\n{decode(train_data[:100].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb072c7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Bigram Model\n",
    "\n",
    "**Inductive Bias: The Markov Assumption**\n",
    "\n",
    "Our first model makes a strong assumption: *the next character depends ONLY on the current character*. This is called a **bigram model** or first-order Markov model.\n",
    "\n",
    "### Why start here?\n",
    "1. It's the simplest possible language model\n",
    "2. We can understand it both as **counting** and as a **neural network**\n",
    "3. Its limitations will motivate our next model\n",
    "4. **Historical context**: Before deep learning, n-gram models (bigrams, trigrams) were the state-of-the-art for language modeling!\n",
    "\n",
    "Let's first see what this looks like as a counting problem, then implement it as a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1508195e",
   "metadata": {},
   "source": [
    "### Understanding Bigrams Through Counting\n",
    "\n",
    "Before building a neural network, let's understand what a bigram model actually learns: **transition probabilities** between characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d334f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count bigram transitions in the training data\n",
    "import collections\n",
    "\n",
    "bigram_counts = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "\n",
    "# Count all character pairs in sequence\n",
    "for i in range(len(train_data) - 1):\n",
    "    current_char = train_data[i].item()\n",
    "    next_char = train_data[i + 1].item()\n",
    "    bigram_counts[current_char][next_char] += 1\n",
    "\n",
    "# Show some examples\n",
    "print(\"Sample Bigram Counts (what follows 't'):\")\n",
    "print(\"-\" * 40)\n",
    "t_idx = stoi['t']\n",
    "t_transitions = bigram_counts[t_idx]\n",
    "top_5 = sorted(t_transitions.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "for next_char_idx, count in top_5:\n",
    "    next_char = itos[next_char_idx]\n",
    "    print(f\"  't' → '{next_char}': {count:,} times\")\n",
    "\n",
    "print(\"\\nKey Insight: A bigram model learns these transition frequencies.\")\n",
    "print(\"We can represent this as a matrix of probabilities!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9346fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create proper sequential bigram training data\n",
    "def create_bigram_dataset(data):\n",
    "    \"\"\"\n",
    "    Creates sequential bigram pairs from the data.\n",
    "    X[i] is a character, Y[i] is the character that follows it.\n",
    "    \"\"\"\n",
    "    X = data[:-1]  # All characters except the last\n",
    "    Y = data[1:]   # All characters except the first\n",
    "    return X, Y\n",
    "\n",
    "# Create train and validation datasets\n",
    "X_train_bigram, Y_train_bigram = create_bigram_dataset(train_data)\n",
    "X_val_bigram, Y_val_bigram = create_bigram_dataset(val_data)\n",
    "\n",
    "print(f\"Training examples: {len(X_train_bigram):,}\")\n",
    "print(f\"Validation examples: {len(X_val_bigram):,}\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\n--- Sample Bigram Pairs (X → Y) ---\")\n",
    "for i in range(10):\n",
    "    x_char = decode([X_train_bigram[i].item()])\n",
    "    y_char = decode([Y_train_bigram[i].item()])\n",
    "    print(f\"'{x_char}' → '{y_char}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009ff7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create batches\n",
    "def get_batch_bigram(split, batch_size=32):\n",
    "    \"\"\"Samples a random batch of bigram pairs.\"\"\"\n",
    "    X = X_train_bigram if split == 'train' else X_val_bigram\n",
    "    Y = Y_train_bigram if split == 'train' else Y_val_bigram\n",
    "    \n",
    "    # Sample random indices\n",
    "    ix = torch.randint(len(X), (batch_size,), generator=g)\n",
    "    \n",
    "    return X[ix], Y[ix]\n",
    "\n",
    "# Test the batch function\n",
    "X_sample, Y_sample = get_batch_bigram('train', batch_size=5)\n",
    "print(\"Sample batch:\")\n",
    "for x, y in zip(X_sample, Y_sample):\n",
    "    print(f\"  '{decode([x.item()])}' → '{decode([y.item()])}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a49f9e4",
   "metadata": {},
   "source": [
    "### Bigram Neural Network Architecture\n",
    "\n",
    "Instead of counting, we'll represent the bigram transition probabilities as a **learnable weight matrix** W.\n",
    "\n",
    "- **W** has shape (vocab_size, vocab_size)\n",
    "- **W[i, j]** represents the \"score\" for character j following character i\n",
    "- We'll use gradient descent to learn these scores from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273bdb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights: A tensor of size (vocab_size, vocab_size)\n",
    "# W[i, j] stores the likelihood of character j following character i\n",
    "W = torch.randn((vocab_size, vocab_size), generator=g, requires_grad=True)\n",
    "\n",
    "print(\"--- The Bigram Weight Matrix (W) ---\")\n",
    "print(f\"Shape of W: {W.shape}\")\n",
    "print(f\"Total parameters: {W.numel():,}\")\n",
    "print(f\"W requires gradients: {W.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f046e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Pass and Loss Calculation\n",
    "X, Y = get_batch_bigram('train')\n",
    "\n",
    "# 1. Input Encoding: Convert character indices to one-hot vectors\n",
    "X_encoded = F.one_hot(X, num_classes=vocab_size).float()\n",
    "\n",
    "# 2. Prediction: Matrix multiplication to get logits\n",
    "logits = X_encoded @ W \n",
    "\n",
    "# 3. Loss Calculation (Cross Entropy combines softmax and negative log likelihood)\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Initial loss (untrained): {loss.item():.4f}\")\n",
    "print(f\"\\nExpected random loss: {torch.log(torch.tensor(vocab_size)).item():.4f}\")\n",
    "print(\"(This is -log(1/vocab_size), the loss of random guessing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32909bfc",
   "metadata": {},
   "source": [
    "### Training the Bigram Model\n",
    "\n",
    "We'll track both training and validation loss to monitor learning progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5382e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop with validation tracking\n",
    "epochs = 400\n",
    "learning_rate = 50\n",
    "\n",
    "# Storage for plotting\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Starting Bigram Model Training...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for k in range(epochs):\n",
    "    # === Training Step ===\n",
    "    X, Y = get_batch_bigram('train')\n",
    "    X_encoded = F.one_hot(X, num_classes=vocab_size).float()\n",
    "    logits = X_encoded @ W \n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    \n",
    "    # Backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    W.data += -learning_rate * W.grad\n",
    "    \n",
    "    # === Evaluation (no gradient computation) ===\n",
    "    if k % 100 == 0:\n",
    "        with torch.no_grad():\n",
    "            # Training loss\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # Validation loss\n",
    "            X_val, Y_val = get_batch_bigram('val', batch_size=1000)\n",
    "            X_val_encoded = F.one_hot(X_val, num_classes=vocab_size).float()\n",
    "            logits_val = X_val_encoded @ W\n",
    "            val_loss = F.cross_entropy(logits_val, Y_val)\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "            print(f\"Epoch {k:4d} | Train Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391c6bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(0, epochs, 100), train_losses, label='Training Loss', marker='o')\n",
    "plt.plot(range(0, epochs, 100), val_losses, label='Validation Loss', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Bigram Model: Learning Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ The model has learned the bigram transition probabilities!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e454c1ff",
   "metadata": {},
   "source": [
    "### Generating Text with the Bigram Model\n",
    "\n",
    "Let's see what kind of text our bigram model produces.\n",
    "\n",
    "**Note on Temperature:** The `temperature` parameter controls randomness:\n",
    "- temperature = 1.0: Use the model's probabilities as-is\n",
    "- temperature < 1.0: Make the model more confident (less random)\n",
    "- temperature > 1.0: Make the model more exploratory (more random)\n",
    "\n",
    "When ChatGPT and similar models ask you for a temperature, this is what it means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f442db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_gen = torch.Generator().manual_seed(67)\n",
    "# Text generation function for bigram model\n",
    "def generate_bigram(max_tokens=200, temperature=1.0):\n",
    "    \"\"\"Generate text using the trained bigram model.\"\"\"\n",
    "    \n",
    "    # Start with a random character\n",
    "    current = torch.randint(1, vocab_size, (1,), generator=g_gen)\n",
    "    result = [current.item()]\n",
    "    \n",
    "    for _ in range(max_tokens):\n",
    "        # Get logits for current character\n",
    "        x_enc = F.one_hot(current, num_classes=vocab_size).float()\n",
    "        logits = x_enc @ W\n",
    "        # Apply temperature and sample\n",
    "        probs = F.softmax(logits / temperature, dim=1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1, generator=g_gen).reshape(-1)\n",
    "        result.append(next_token.item())\n",
    "        current = next_token\n",
    "    \n",
    "    return decode(result)\n",
    "\n",
    "print(\"--- Bigram Model Generated Text ---\\n\")\n",
    "print(generate_bigram(300))\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f41386",
   "metadata": {},
   "source": [
    "### Reflection: Limitations of the Bigram Model\n",
    "\n",
    "Look at the generated text. You'll notice:\n",
    "- Individual character transitions look reasonable (e.g., 'in', 'be', ':' followed by newline, capital letters grouped up)\n",
    "- But there are no real words or coherent structure\n",
    "- The model has **no memory** beyond the immediate previous character\n",
    "\n",
    "**Why?** Our inductive bias is too restrictive! Real language has longer-range dependencies:\n",
    "- \"The cat sat on the ___\" → \"mat\" (depends on \"cat\", not just \"e\")\n",
    "- \"She said she would ___\" → needs to remember \"she\"\n",
    "\n",
    "**Historical note**: This limitation is exactly why researchers moved beyond n-gram models to neural networks with longer context windows, and eventually to transformers with attention mechanisms.\n",
    "\n",
    "**Question:** How can we give the model more context?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c542e39",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: MLP with Context Window\n",
    "\n",
    "**New Inductive Bias: Fixed Context Window**\n",
    "\n",
    "Let's improve our model by allowing it to look at the previous **N characters** (not just 1) to predict the next character.\n",
    "\n",
    "**Design Choice #2: Context Window Size**\n",
    "- Bigram: context = 1 character\n",
    "- Our new model: context = 3 characters (we'll call this `block_size`)\n",
    "\n",
    "This is still a simplification (real language has much longer dependencies), but it's a step forward!\n",
    "\n",
    "**Connection to LLMs**: Modern models like GPT-4 have context windows of (hundreds of) thousands of tokens. The principle is the same, we're just scaling up! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a474ba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define context window size\n",
    "block_size = 3 \n",
    "print(f\"New Inductive Bias: Predict next character using previous {block_size} characters\")\n",
    "print(f\"Example: Given 'cat', predict what comes next\")\n",
    "\n",
    "# Create the context-based dataset\n",
    "def create_dataset(data):\n",
    "    \"\"\"Creates input (X) and target (Y) tensors based on block_size.\"\"\"\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - block_size):\n",
    "        context = data[i:i + block_size] \n",
    "        target = data[i + block_size]\n",
    "        X.append(context)\n",
    "        Y.append(target)\n",
    "    \n",
    "    X = torch.stack(X)\n",
    "    Y = torch.stack(Y)\n",
    "    return X, Y\n",
    "\n",
    "# Generate datasets\n",
    "X_train, Y_train = create_dataset(train_data)\n",
    "X_val, Y_val = create_dataset(val_data)\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"Training examples: {len(X_train):,}\")\n",
    "print(f\"Validation examples: {len(X_val):,}\")\n",
    "print(f\"\\nX_train shape: {X_train.shape} (num_examples, context_length)\")\n",
    "print(f\"Y_train shape: {Y_train.shape}\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\n--- Sample Context → Target ---\")\n",
    "for i in range(5):\n",
    "    context = decode(X_train[i].tolist())\n",
    "    target = decode([Y_train[i].item()])\n",
    "    print(f\"'{context}' → '{target}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8819b6b9",
   "metadata": {},
   "source": [
    "### Design Choice #3: Character Embeddings\n",
    "\n",
    "**Problem with One-Hot Encoding:**\n",
    "- Each character is represented as a sparse vector (e.g., [0,0,1,0,...,0])\n",
    "- Characters are treated as completely independent\n",
    "- No notion of similarity (e.g., vowels, consonants)\n",
    "\n",
    "**Solution: Learned Embeddings**\n",
    "- Represent each character as a dense vector (e.g., 10 dimensions)\n",
    "- The model learns these representations during training\n",
    "- Similar characters (appearing in similar contexts) develop similar embeddings\n",
    "\n",
    "**This is a key innovation in modern NLP!** Word embeddings (Word2Vec, GloVe) revolutionized NLP in the 2010s. Modern LLMs use the same principle but learn embeddings for subword tokens. GPT-3, for example, uses 12,288-dimensional embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8eca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding hyperparameter\n",
    "embed_dim = 10\n",
    "\n",
    "# Create the embedding matrix\n",
    "# Each character gets a 10-dimensional vector\n",
    "C = torch.randn((vocab_size, embed_dim), generator=g)\n",
    "\n",
    "print(f\"Embedding Matrix C shape: {C.shape}\")\n",
    "print(f\"Each character is now represented by a {embed_dim}-dimensional vector\")\n",
    "\n",
    "# Example: Look up embeddings for a sample\n",
    "sample_context = X_train[0]  # e.g., [15, 23, 8]\n",
    "sample_embeddings = C[sample_context]\n",
    "\n",
    "print(f\"\\nSample context indices: {sample_context.tolist()}\")\n",
    "print(f\"Sample context text: '{decode(sample_context.tolist())}'\")\n",
    "print(f\"Embeddings shape: {sample_embeddings.shape} (context_length, embed_dim)\")\n",
    "print(f\"\\nFirst character embedding:\\n{sample_embeddings[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb203b",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron (MLP) Architecture\n",
    "\n",
    "Now we need to combine the information from our 3 character embeddings to predict the next character.\n",
    "\n",
    "**Architecture:**\n",
    "1. **Input:** 3 characters → 3 embeddings of size 10 → flatten to vector of size 30\n",
    "2. **Hidden Layer:** Linear transformation (30 → 100) + Tanh activation\n",
    "3. **Output Layer:** Linear transformation (100 → vocab_size) → logits\n",
    "\n",
    "The **Tanh activation** allows the model to learn non-linear patterns (e.g., \"qu\" is common, but \"qz\" is not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57ae551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP hyperparameters\n",
    "hidden_size = 100 \n",
    "input_dim = block_size * embed_dim  # 3 * 10 = 30\n",
    "\n",
    "# Initialize parameters with careful scaling\n",
    "W1 = torch.randn((input_dim, hidden_size), generator=g)# * (5/3) / (input_dim**0.5)\n",
    "b1 = torch.randn(hidden_size, generator=g)# * 0.1\n",
    "\n",
    "W2 = torch.randn((hidden_size, vocab_size), generator=g)# * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g)# * 0.1\n",
    "\n",
    "# Collect all parameters\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MLP MODEL ARCHITECTURE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Input: {block_size} characters\")\n",
    "print(f\"Embedding: {vocab_size} × {embed_dim} = {C.numel():,} parameters\")\n",
    "print(f\"Hidden Layer: {input_dim} × {hidden_size} = {W1.numel():,} parameters\")\n",
    "print(f\"Output Layer: {hidden_size} × {vocab_size} = {W2.numel():,} parameters\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in parameters):,}\")\n",
    "print(f\"Bigram model had: {vocab_size**2:,} parameters\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e98b46",
   "metadata": {},
   "source": [
    "### Training the MLP Model\n",
    "\n",
    "We'll use the Adam optimizer (a more sophisticated version of gradient descent) and track both training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ded9464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch sampling function\n",
    "def get_batch_context(split, batch_size=64):\n",
    "    \"\"\"Samples a random batch from the context dataset.\"\"\"\n",
    "    X = X_train if split == 'train' else X_val\n",
    "    Y = Y_train if split == 'train' else Y_val\n",
    "    \n",
    "    ix = torch.randint(len(X), (batch_size,), generator=g)\n",
    "    return X[ix], Y[ix]\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.01)\n",
    "epochs = 15000\n",
    "\n",
    "# Storage for plotting\n",
    "mlp_train_losses = []\n",
    "mlp_val_losses = []\n",
    "\n",
    "print(\"Starting MLP Context Model Training...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for k in range(epochs):\n",
    "    # === Training Step ===\n",
    "    Xb, Yb = get_batch_context('train')\n",
    "    \n",
    "    # Forward pass\n",
    "    emb = C[Xb]  # (batch, block_size, embed_dim)\n",
    "    emb_flat = emb.view(emb.shape[0], -1)  # (batch, block_size * embed_dim)\n",
    "    h = torch.tanh(emb_flat @ W1 + b1)  # (batch, hidden_size)\n",
    "    logits = h @ W2 + b2  # (batch, vocab_size)\n",
    "    \n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # === Evaluation ===\n",
    "    if k % 1000 == 0:\n",
    "        with torch.no_grad():\n",
    "            mlp_train_losses.append(loss.item())\n",
    "            \n",
    "            # Validation loss\n",
    "            Xb_val, Yb_val = get_batch_context('val', batch_size=1000)\n",
    "            emb_val = C[Xb_val]\n",
    "            emb_val_flat = emb_val.view(emb_val.shape[0], -1)\n",
    "            h_val = torch.tanh(emb_val_flat @ W1 + b1)\n",
    "            logits_val = h_val @ W2 + b2\n",
    "            val_loss = F.cross_entropy(logits_val, Yb_val)\n",
    "            mlp_val_losses.append(val_loss.item())\n",
    "            \n",
    "            print(f\"Epoch {k:5d} | Train Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Final Training Loss: {mlp_train_losses[-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {mlp_val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397a8147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MLP learning curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(0, epochs, 1000), mlp_train_losses, label='Training Loss', marker='o')\n",
    "plt.plot(range(0, epochs, 1000), mlp_val_losses, label='Validation Loss', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('MLP Context Model: Learning Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ac3ab6",
   "metadata": {},
   "source": [
    "### Generating Text with the MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb4a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mlp(max_new_tokens=300, temperature=1.0):\n",
    "    \"\"\"Generate text using the trained MLP model.\"\"\"\n",
    "    # Start with padding tokens\n",
    "    context = [0] * block_size\n",
    "    result = []\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Forward pass\n",
    "        X_context = torch.tensor([context], dtype=torch.long)\n",
    "        emb = C[X_context]\n",
    "        emb_flat = emb.view(1, -1)\n",
    "        h = torch.tanh(emb_flat @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        \n",
    "        # Sample next token\n",
    "        probs = F.softmax(logits / temperature, dim=1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1, generator=g_gen).item()\n",
    "        \n",
    "        # Update context (sliding window)\n",
    "        context = context[1:] + [next_token]\n",
    "        result.append(next_token)\n",
    "    \n",
    "    return decode(result).replace('<pad>', '')\n",
    "\n",
    "print(\"--- MLP Model Generated Text ---\\n\")\n",
    "print(generate_mlp(300))\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b3ef23",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interlude: Visualizing What the Model Learned\n",
    "\n",
    "Before comparing our models, let's peek inside the MLP to see what it actually learned. The embedding layer should have discovered meaningful structure in the character space.\n",
    "\n",
    "### Extra Credit: 2D Embedding Visualization\n",
    "\n",
    "To visualize embeddings, we need them to be 2-dimensional. Let's quickly train a version of our MLP with `embed_dim=2` so we can plot the character embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb77043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a 2D embedding version for visualization\n",
    "print(\"Training a 2D embedding model for visualization...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Hyperparameters (same as before, but embed_dim=2)\n",
    "embed_dim_2d = 2\n",
    "hidden_size_2d = 100\n",
    "input_dim_2d = block_size * embed_dim_2d\n",
    "\n",
    "# Initialize parameters\n",
    "g_viz = torch.Generator().manual_seed(67)  # Same seed for reproducibility\n",
    "C_2d = torch.randn((vocab_size, embed_dim_2d), generator=g_viz)\n",
    "W1_2d = torch.randn((input_dim_2d, hidden_size_2d), generator=g_viz)# * (5/3) / (input_dim_2d**0.5)\n",
    "b1_2d = torch.randn(hidden_size_2d, generator=g_viz)# * 0.1\n",
    "W2_2d = torch.randn((hidden_size_2d, vocab_size), generator=g_viz)# * 0.1\n",
    "b2_2d = torch.randn(vocab_size, generator=g_viz)# * 0.1\n",
    "\n",
    "parameters_2d = [C_2d, W1_2d, b1_2d, W2_2d, b2_2d]\n",
    "for p in parameters_2d:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Train (fewer epochs for speed)\n",
    "optimizer_2d = torch.optim.Adam(parameters_2d, lr=0.01)\n",
    "epochs_2d = 20000\n",
    "\n",
    "for k in range(epochs_2d):\n",
    "    Xb, Yb = get_batch_context('train')\n",
    "    \n",
    "    emb = C_2d[Xb]\n",
    "    emb_flat = emb.view(emb.shape[0], -1)\n",
    "    h = torch.tanh(emb_flat @ W1_2d + b1_2d)\n",
    "    logits = h @ W2_2d + b2_2d\n",
    "    \n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "    optimizer_2d.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer_2d.step()\n",
    "    \n",
    "    if k % 1000 == 0:\n",
    "        print(f\"Epoch {k:4d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"✓ 2D model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c14dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the 2D embeddings\n",
    "import numpy as np\n",
    "\n",
    "# Extract embeddings\n",
    "embeddings_2d = C_2d.detach().numpy()\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.3, s=100)\n",
    "\n",
    "# Annotate each point with its character\n",
    "for i in range(vocab_size):\n",
    "    char = itos[i]\n",
    "    x, y = embeddings_2d[i]\n",
    "    \n",
    "    # Use different colors for different character types\n",
    "    if char == '<pad>':\n",
    "        color = 'red'\n",
    "        fontsize = 10\n",
    "    elif char in 'aeiouAEIOU':\n",
    "        color = 'blue'\n",
    "        fontsize = 12\n",
    "    elif char in '.,;:!?$\\n':\n",
    "        color = 'green'\n",
    "        fontsize = 10\n",
    "    elif char == ' ':\n",
    "        color = 'orange'\n",
    "        fontsize = 10\n",
    "        char = '␣'  # Visible space character\n",
    "    elif char.isupper():\n",
    "        color = 'purple'\n",
    "        fontsize = 11\n",
    "    else:\n",
    "        color = 'black'\n",
    "        fontsize = 11\n",
    "    \n",
    "    plt.annotate(char, (x, y), fontsize=fontsize, color=color, \n",
    "                ha='center', va='center', weight='bold')\n",
    "\n",
    "plt.xlabel('Embedding Dimension 1', fontsize=12)\n",
    "plt.ylabel('Embedding Dimension 2', fontsize=12)\n",
    "plt.title('2D Character Embeddings: Learned Representations', fontsize=14, weight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='blue', label='Vowels'),\n",
    "    Patch(facecolor='purple', label='Uppercase'),\n",
    "    Patch(facecolor='green', label='Punctuation'),\n",
    "    Patch(facecolor='orange', label='Space'),\n",
    "    Patch(facecolor='black', label='Other lowercase'),\n",
    "    Patch(facecolor='red', label='Special token')\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc='best')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ad8a88",
   "metadata": {},
   "source": [
    "### What Do You Notice?\n",
    "\n",
    "Look at the embedding space visualization above. You should see interesting patterns:\n",
    "\n",
    "1. **Vowels cluster together** (blue letters) - the model learned they behave similarly. But apparently upper case vowels are closer to other upper case characters than lower case vowels!\n",
    "2. **Punctuation marks group** (green) - they appear in similar contexts\n",
    "3. **Uppercase letters** (purple) might form their own region\n",
    "4. **The space character** (orange ␣) is often in a unique position\n",
    "\n",
    "**Why does this happen?** The model learns to place characters that appear in similar contexts close together in embedding space. For example:\n",
    "- Vowels often follow consonants and precede consonants\n",
    "- Punctuation marks often follow words and precede spaces\n",
    "- Uppercase letters often start sentences\n",
    "\n",
    "**Key Insight:** The model discovered linguistic structure **without being told**! We never told it what a vowel is or what punctuation does. It learned this purely from the statistical patterns in the text.\n",
    "\n",
    "This is the power of learned representations; the model builds its own useful abstractions from data.\n",
    "\n",
    "**Connection to LLMs**: The same thing happens in modern language models! Word embeddings in GPT models capture semantic relationships like \"Paris - France + Italy ≈ Rome\". The model learns these relationships purely from training data steered by inductive biases, without explicit programming.\n",
    "\n",
    "**Reflection:** \n",
    "- Are there any surprising clusters or separations?\n",
    "- What does the position of the space character tell you?\n",
    "- How might this structure help the model predict the next character?\n",
    "- What kinds of relationships might a word-level model learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506880bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side loss comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Bigram losses\n",
    "ax1.plot(range(0, 400, 100), train_losses, label='Train', marker='o')\n",
    "ax1.plot(range(0, 400, 100), val_losses, label='Validation', marker='s')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Bigram Model')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# MLP losses\n",
    "ax2.plot(range(0, epochs, 1000), mlp_train_losses, label='Train', marker='o')\n",
    "ax2.plot(range(0, epochs, 1000), mlp_val_losses, label='Validation', marker='s')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('MLP Context Model')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Bigram Final Val Loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"MLP Final Val Loss: {mlp_val_losses[-1]:.4f}\")\n",
    "print(f\"Improvement: {((val_losses[-1] - mlp_val_losses[-1]) / val_losses[-1] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b3dc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comparison samples\n",
    "print(\"=\" * 70)\n",
    "print(\"SIDE-BY-SIDE TEXT GENERATION COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(\"\\nBigram Model:\")\n",
    "    print(generate_bigram(200))\n",
    "    print(\"\\nMLP Context Model:\")\n",
    "    print(generate_mlp(200))\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2f433f",
   "metadata": {},
   "source": [
    "### Summary: How Design Choices Shape Model Performance\n",
    "\n",
    "| Design Choice | Bigram Model | MLP Context Model | Modern LLMs (e.g., GPT-4) |\n",
    "|--------------|--------------|-------------------|---------------------------|\n",
    "| **Data Representation** | One-hot encoding | Learned embeddings (10D) | Learned embeddings (>12,000D) |\n",
    "| **Architecture** | Single linear layer | Embeddings → MLP | Embeddings → Transformer layers |\n",
    "| **Parameters** | ~4,000 | ~10,000 | >100 billion (GPT-4 estimated at 1.7 trillion) |\n",
    "| **Training Data** | 1MB Shakespeare | 1MB Shakespeare | Trillions of tokens |\n",
    "| **Context Window** | 1 character | 3 characters | 128,000-1,000,000 tokens |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Inductive biases matter**: Allowing the model to see 3 characters instead of 1 dramatically improved performance. As did moving to a stronger model architecture. Modern LLMs extend this to thousands of tokens and gigantic models.\n",
    "\n",
    "2. **Representation matters**: Learned embeddings allow the model to discover that some characters are similar. Modern LLMs learn incredibly rich representations that capture semantic meaning.\n",
    "\n",
    "3. **Scale matters, but principles don't change**: GPT-4 has ~170,000× more parameters than our model, but it uses the same fundamental ideas:\n",
    "   - Next-token prediction\n",
    "   - Learned embeddings\n",
    "   - Gradient descent optimization\n",
    "   - Cross-entropy loss\n",
    "\n",
    "4. **Machine learning is modeling**: We made explicit choices about:\n",
    "   - What data to use (Shakespeare text)\n",
    "   - How to represent it (character-level, embeddings)\n",
    "   - What assumptions to make (context window size)\n",
    "   - What architecture to use (MLP with non-linearity)\n",
    "\n",
    "These choices fundamentally shaped what our models could learn! The same is true for modern LLMs—their capabilities emerge from careful design choices about data, architecture, and training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion Questions\n",
    "\n",
    "1. **Context Window**: We used `block_size=3`. What would happen with `block_size=1`? With `block_size=10`? What are the trade-offs?\n",
    "\n",
    "2. **Embedding Dimension**: We used `embed_dim=10` for the main model and `embed_dim=2` for visualization. How might using `embed_dim=50` or `embed_dim=100` affect the model? What about `embed_dim=1`?\n",
    "\n",
    "3. **Architecture Depth**: Our MLP has one hidden layer. What might we gain from adding more layers? What might we lose? (Hint: GPT-3 has 96 layers!)\n",
    "\n",
    "4. **Data Source**: We used Shakespeare. How would the model differ if we trained on:\n",
    "   - Python code?\n",
    "   - Modern English novels?\n",
    "   - Social media posts?\n",
    "   - A different language (e.g., Chinese, Arabic)?\n",
    "   \n",
    "   What does this tell you about how LLMs trained on internet-scale data might behave?\n",
    "\n",
    "5. **Limitations**: Both models still produce mostly gibberish. What fundamental limitations do they have? (Hint: think about long-range dependencies like \"The cat that chased the mouse ___ tired\")\n",
    "\n",
    "6. **Scaling Laws**: Our MLP performs better than the bigram model. If we kept scaling up (more parameters, more data, more compute), would performance keep improving? This is an active research question in modern AI!\n",
    "\n",
    "### From Here to ChatGPT: What's Missing?\n",
    "\n",
    "Our models demonstrate the core principles, but modern LLMs add several key innovations:\n",
    "\n",
    "1. **Attention Mechanisms**: Instead of a fixed context window, attention allows the model to focus on relevant parts of the input dynamically. This is the \"T\" in GPT (Generative Pre-trained **Transformer**).\n",
    "\n",
    "2. **Scale**: \n",
    "   - **More parameters**: GPT-3 has 175 billion parameters vs. our ~10,000\n",
    "   - **More data**: Trained on hundreds of billions of tokens vs. our 1 million characters\n",
    "   - **More compute**: Thousands of GPUs for weeks vs. our CPU for minutes\n",
    "\n",
    "3. **Pre-training + Fine-tuning**: LLMs are first trained on massive text corpora (pre-training), then fine-tuned on specific tasks or with human feedback (RLHF).\n",
    "\n",
    "4. **Architectural improvements**: Layer normalization, residual connections, better optimizers, etc.\n",
    "\n",
    "But the **fundamental task remains the same**: predict the next token. Everything else is about doing this task better at scale!\n",
    "\n",
    "### Optional Extensions\n",
    "\n",
    "If you want to explore further:\n",
    "\n",
    "1. **Experiment with hyperparameters**: Try different `block_size`, `embed_dim`, or `hidden_size` values\n",
    "2. **Temperature sampling**: Generate text with different temperature values (0.5, 1.0, 1.5) and compare\n",
    "3. **Different datasets**: Train on a different text corpus and compare the learned embeddings\n",
    "4. **Longer training**: Train for more epochs and see if the text quality improves\n",
    "5. **3D embeddings**: Train with `embed_dim=3` and create a 3D visualization using `mpl_toolkits.mplot3d`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introduction-to-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
