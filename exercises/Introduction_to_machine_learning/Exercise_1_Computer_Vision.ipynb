{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b310e99",
   "metadata": {},
   "source": [
    "# Exercise 1: Image Classification and Modeling Choices\n",
    "\n",
    "In this exercise, we'll explore how different **modeling choices** affect performance on image classification tasks. Machine learning is a *modeling science*, and this means that we have to make informed decisions about what is important and what isn't in our data. The choices we're engaging with in this exercise are:\n",
    "\n",
    "- **Data representation**: How do we structure our input?\n",
    "- **Inductive biases**: What assumptions do we build into our model?\n",
    "- **Data augmentation**: Can we simulate data that *exemplify* important features of the problem?\n",
    "\n",
    "We'll work with two datasets:\n",
    "- **FashionMNIST**: 28×28 grayscale images of clothing items (10 classes)\n",
    "- **CIFAR-10**: 32×32 color images of objects (10 classes)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this exercise, you should understand:\n",
    "1. Why a linear model treats images as unstructured vectors\n",
    "2. How CNNs encode spatial structure as an inductive bias\n",
    "3. When and why data augmentation helps\n",
    "4. How modeling choices interact with dataset characteristics\n",
    "5. How to detect overfitting using train/validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195e48f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and configuration\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Device and reproducibility\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(67)\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.manual_seed_all(67)\n",
    "\n",
    "# Human-readable class names for FashionMNIST\n",
    "class_names = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a69da66",
   "metadata": {},
   "source": [
    "## Part 1: Loading and Exploring FashionMNIST\n",
    "\n",
    "Let's start by loading our first dataset to understand what we're working with.\n",
    "\n",
    "**Important**: We'll split our data into three sets:\n",
    "- **Training set**: Used to update model weights\n",
    "- **Validation set**: Used to monitor overfitting and tune hyperparameters\n",
    "- **Test set**: Used only for final evaluation (never seen during training!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae9770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preprocessing (FashionMNIST)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.0,), (0.5,))  # Normalize to [-1, 1] range\n",
    "])\n",
    "\n",
    "data_dir = './data'\n",
    "\n",
    "# Load full training set\n",
    "full_train_dataset = torchvision.datasets.FashionMNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "\n",
    "# Split into train (48k) and validation (12k) - 80/20 split\n",
    "train_size = 48000\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size], \n",
    "                                          generator=torch.Generator().manual_seed(67))\n",
    "\n",
    "# Load test set (10k samples)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "# Batch size: Number of samples processed before updating weights\n",
    "# Larger = faster training but more memory; smaller = more stable but slower\n",
    "# 64 is a common choice balancing speed and stability\n",
    "batch_size = 64\n",
    "\n",
    "# num_workers: Parallel data loading processes (adjust based on your CPU cores)\n",
    "num_workers = 4\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "print(f'Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}, Test samples: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f7f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize a few examples to understand our data\n",
    "images, labels = next(iter(train_loader))\n",
    "print('Batch shapes:', images.shape, labels.shape)  # (batch_size, 1, 28, 28)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "for i in range(8):\n",
    "    plt.subplot(1, 8, i+1)\n",
    "    plt.imshow(images[i].squeeze().numpy(), cmap='gray')\n",
    "    plt.title(class_names[labels[i]], fontsize=9)\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Sample FashionMNIST Images', fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd42dfa3",
   "metadata": {},
   "source": [
    "## Part 2: Baseline Linear Model\n",
    "\n",
    "**The simplest approach**: Treat each image as a flat vector of 784 pixels (28×28) and learn a linear mapping to 10 classes.\n",
    "\n",
    "**Key question**: What does this model assume about images? Does it know that nearby pixels are related? Does it understand that a shirt in the top-left corner is the same as a shirt in the center?\n",
    "\n",
    "**Spoiler**: No! A linear model treats each pixel position as an independent feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aea1f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=784, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # flatten to (batch_size, 784)\n",
    "        return self.fc(x)\n",
    "\n",
    "linear_model = LinearClassifier().to(device)\n",
    "print(f\"Linear model parameters: {sum(p.numel() for p in linear_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f97767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation helpers\n",
    "def evaluate(model, loader):\n",
    "    \"\"\"Compute accuracy on a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labs in loader:\n",
    "            imgs, labs = imgs.to(device), labs.to(device)\n",
    "            outputs = model(imgs)\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += (preds == labs).sum().item()\n",
    "            total += labs.size(0)\n",
    "    return 100.0 * correct / total\n",
    "\n",
    "def compute_loss(model, loader, criterion):\n",
    "    \"\"\"Compute average loss on a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labs in loader:\n",
    "            imgs, labs = imgs.to(device), labs.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labs)\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            total_samples += imgs.size(0)\n",
    "    return total_loss / total_samples\n",
    "\n",
    "def get_predictions(model, loader):\n",
    "    \"\"\"Get all predictions and true labels for confusion matrix.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labs in loader:\n",
    "            imgs, labs = imgs.to(device), labs.to(device)\n",
    "            outputs = model(imgs)\n",
    "            _, preds = outputs.max(1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labs.cpu().numpy())\n",
    "    return np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "def plot_confusion_matrix(model, loader, class_names, title=\"Confusion Matrix\"):\n",
    "    \"\"\"Plot a confusion matrix for the model's predictions.\"\"\"\n",
    "    y_true, y_pred = get_predictions(model, loader)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Normalize to percentages\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Percentage (%)'})\n",
    "    plt.title(title, fontsize=14, pad=20)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cm\n",
    "\n",
    "def plot_training_curves(history, title=\"Training History\"):\n",
    "    \"\"\"Plot training and validation loss curves.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1.plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "    ax1.plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch', fontsize=11)\n",
    "    ax1.set_ylabel('Loss', fontsize=11)\n",
    "    ax1.set_title('Loss Curves', fontsize=12)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    ax2.plot(history['val_acc'], label='Val Accuracy', linewidth=2, color='green')\n",
    "    ax2.set_xlabel('Epoch', fontsize=11)\n",
    "    ax2.set_ylabel('Accuracy (%)', fontsize=11)\n",
    "    ax2.set_title('Validation Accuracy', fontsize=12)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=5, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Train a model and report progress.\n",
    "    \n",
    "    Args:\n",
    "        epochs: Number of complete passes through training data\n",
    "                More epochs = more learning, but risk of overfitting\n",
    "        lr: Learning rate - controls step size during optimization\n",
    "            Too high = unstable training; too low = slow convergence\n",
    "            1e-3 (0.001) is a common starting point for Adam optimizer\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Track metrics for plotting\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for imgs, labs in train_loader:\n",
    "            imgs, labs = imgs.to(device), labs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "        \n",
    "        # Compute metrics\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        val_loss = compute_loss(model, val_loader, criterion)\n",
    "        val_acc = evaluate(model, val_loader)\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch:02d} — Train Loss: {train_loss:.4f} — Val Loss: {val_loss:.4f} — Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Train the linear baseline\n",
    "print(\"Training linear model...\")\n",
    "linear_model, linear_history = train_model(linear_model, train_loader, val_loader, epochs=10)\n",
    "linear_acc = evaluate(linear_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bf0153",
   "metadata": {},
   "source": [
    "### Detecting Overfitting: Training Curves\n",
    "\n",
    "Before looking at the confusion matrix, let's examine the **training curves**. These are crucial for understanding how well your model generalizes.\n",
    "\n",
    "**What to look for:**\n",
    "- **Train loss decreasing**: Model is learning from training data ✓\n",
    "- **Val loss decreasing**: Model generalizes to new data ✓\n",
    "- **Val loss increasing while train loss decreases**: **OVERFITTING!** ⚠️\n",
    "  - The model is memorizing training data instead of learning patterns\n",
    "  - Solution: Stop training earlier, add regularization, or get more data\n",
    "- **Gap between train and val loss**: Some gap is normal, but large gaps indicate overfitting\n",
    "\n",
    "Let's see how our linear model performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8271cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for linear model\n",
    "plot_training_curves(linear_history, title=\"Linear Model Training History (FashionMNIST)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57087b9",
   "metadata": {},
   "source": [
    "### Understanding Model Predictions: The Confusion Matrix\n",
    "\n",
    "Now let's introduce another diagnostic tool: the **confusion matrix**.\n",
    "\n",
    "**What is it?**  \n",
    "A confusion matrix shows where your model gets confused. Each row represents the true class, and each column represents what the model predicted.\n",
    "\n",
    "**How to read it:**\n",
    "- **Diagonal elements** (top-left to bottom-right): Correct predictions. Higher is better!\n",
    "- **Off-diagonal elements**: Mistakes. The value at row *i*, column *j* tells you how often class *i* was misclassified as class *j*.\n",
    "- **Patterns to look for:**\n",
    "  - Are certain classes systematically confused? (e.g., \"Shirt\" vs \"T-shirt/top\")\n",
    "  - Does the model have a bias toward predicting certain classes?\n",
    "  - Are mistakes symmetric? (Does the model confuse A→B as often as B→A?)\n",
    "\n",
    "Let's see where our linear model struggles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c886fcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for linear model\n",
    "plot_confusion_matrix(linear_model, test_loader, class_names, \n",
    "                     title=\"Linear Model Confusion Matrix (FashionMNIST)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a6cb76",
   "metadata": {},
   "source": [
    "### Error Analysis: Hardest-to-Classify Images\n",
    "\n",
    "Let's look at the images our model struggles with most. This can reveal:\n",
    "- Ambiguous cases even humans would find difficult\n",
    "- Systematic biases in the model\n",
    "- Data quality issues\n",
    "\n",
    "We'll find images where the model was most confident but wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6dd6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_hardest_examples(model, loader, class_names, num_examples=16):\n",
    "    \"\"\"Show the hardest-to-classify examples (high confidence, wrong prediction).\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Collect all predictions with confidence scores\n",
    "    all_errors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labs in loader:\n",
    "            imgs, labs = imgs.to(device), labs.to(device)\n",
    "            outputs = model(imgs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            confidences, preds = probs.max(1)\n",
    "            \n",
    "            # Find incorrect predictions\n",
    "            incorrect = preds != labs\n",
    "            \n",
    "            # Store errors with their confidence\n",
    "            for i in range(len(imgs)):\n",
    "                if incorrect[i]:\n",
    "                    all_errors.append({\n",
    "                        'image': imgs[i].cpu(),\n",
    "                        'true_label': labs[i].item(),\n",
    "                        'pred_label': preds[i].item(),\n",
    "                        'confidence': confidences[i].item()\n",
    "                    })\n",
    "    \n",
    "    # Sort by confidence (descending) - most confident mistakes first\n",
    "    all_errors.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    # Plot top errors\n",
    "    num_to_show = min(num_examples, len(all_errors))\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for i in range(num_to_show):\n",
    "        error = all_errors[i]\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        \n",
    "        # Handle grayscale vs RGB\n",
    "        img = error['image'].squeeze()\n",
    "        if img.ndim == 3:  # RGB\n",
    "            img = img.permute(1, 2, 0).numpy()\n",
    "            img = np.clip(img, 0, 1)\n",
    "            plt.imshow(img)\n",
    "        else:  # Grayscale\n",
    "            plt.imshow(img.numpy(), cmap='gray')\n",
    "        \n",
    "        true_name = class_names[error['true_label']]\n",
    "        pred_name = class_names[error['pred_label']]\n",
    "        conf = error['confidence'] * 100\n",
    "        \n",
    "        plt.title(f'True: {true_name}\\nPred: {pred_name} ({conf:.0f}%)', \n",
    "                 fontsize=8, color='red')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle('Hardest-to-Classify Examples (High Confidence Errors)', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show hardest examples for linear model\n",
    "show_hardest_examples(linear_model, test_loader, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f139be",
   "metadata": {},
   "source": [
    "### Reflection: Linear Model Performance\n",
    "\n",
    "The linear model achieves around **~83-84% accuracy**. Not bad for such a simple approach!\n",
    "\n",
    "But think about what it's learning: it's finding patterns like \"if pixel 234 is bright and pixel 567 is dark, it's probably a sneaker.\" It has no concept of shapes, edges, or spatial relationships.\n",
    "\n",
    "**Looking at the training curves**: Notice that train and validation losses track closely together - this model is **not overfitting** significantly. Why? Because it's too simple to memorize the training data!\n",
    "\n",
    "**Looking at the confusion matrix**, you'll likely notice:\n",
    "- **Shirt vs. T-shirt/top**: These are often confused because they're visually similar\n",
    "- **Trousers and ankle boots**: These are rarely confused with other classes - why?\n",
    "\n",
    "**Looking at the hardest examples**: You'll see that even the model's most confident mistakes are often genuinely ambiguous cases.\n",
    "\n",
    "**Question to consider**: Why does this work at all? What makes FashionMNIST amenable to this approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea599f31",
   "metadata": {},
   "source": [
    "## Part 3: Convolutional Neural Network (CNN)\n",
    "\n",
    "Now let's refine our **inductive bias**. We want to exploit that images have spatial structure.\n",
    "\n",
    "**Key ideas**:\n",
    "- **Convolutions**: Look at local patches of pixels, not individual pixels\n",
    "- **Translation invariance**: A sneaker is a sneaker whether it's in the top-left or bottom-right\n",
    "- **Hierarchical features**: Early layers detect edges, later layers detect shapes\n",
    "\n",
    "We're choosing a *model architecture* based on our preconceived notions of the data and how they relate to the task. This is one of the core principles of machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=10):\n",
    "        super().__init__()\n",
    "        # Convolutional feature extractor\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),  # 32 filters, 3×3 kernels\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # downsample by 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.AdaptiveAvgPool2d((7, 7))  # ensure fixed size for classifier\n",
    "        )\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "cnn_model = SimpleCNN(in_channels=1).to(device)\n",
    "print(f\"CNN parameters: {sum(p.numel() for p in cnn_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89537a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CNN\n",
    "print(\"Training CNN...\")\n",
    "cnn_model, cnn_history = train_model(cnn_model, train_loader, val_loader, epochs=10, lr=1e-3)\n",
    "cnn_acc = evaluate(cnn_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9bae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for CNN\n",
    "plot_training_curves(cnn_history, title=\"CNN Training History (FashionMNIST)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786cbcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for CNN on FashionMNIST\n",
    "plot_confusion_matrix(cnn_model, test_loader, class_names,\n",
    "                     title=\"CNN Confusion Matrix (FashionMNIST)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show hardest examples for CNN\n",
    "show_hardest_examples(cnn_model, test_loader, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba55ca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "cnn_model.eval()\n",
    "imgs, labs = next(iter(test_loader))\n",
    "imgs, labs = imgs.to(device), labs.to(device)\n",
    "with torch.no_grad():\n",
    "    outs = cnn_model(imgs[:32])\n",
    "    preds = outs.argmax(dim=1)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(32):\n",
    "    plt.subplot(4, 8, i+1)\n",
    "    plt.imshow(imgs[i].cpu().squeeze(), cmap='gray')\n",
    "    correct = '✓' if preds[i] == labs[i] else '✗'\n",
    "    plt.title(f'{correct} {class_names[preds[i]]}', fontsize=9, \n",
    "              color='green' if preds[i] == labs[i] else 'red')\n",
    "    plt.axis('off')\n",
    "plt.suptitle('CNN Predictions on FashionMNIST', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec23e70b",
   "metadata": {},
   "source": [
    "### Reflection: The importance of choosing the right inductive bias\n",
    "\n",
    "The CNN achieves around **91-92% accuracy** - a significant improvement over the linear model!\n",
    "\n",
    "**Why?** Because we made and encoded some good assumptions:\n",
    "- Nearby pixels are related (convolutions)\n",
    "- Features can appear anywhere (translation invariance)\n",
    "- Complex patterns are built from simpler ones (hierarchical layers)\n",
    "\n",
    "**Looking at the training curves**: The CNN shows a slightly larger gap between train and validation loss compared to the linear model. This is expected - more powerful models can fit training data better, but we're not seeing severe overfitting yet.\n",
    "\n",
    "**Comparing the confusion matrices**: Notice how the CNN's confusion matrix has stronger diagonal values and weaker off-diagonal values. The CNN still confuses similar items (Shirt/T-shirt, Pullover/Coat), but less frequently. This shows that spatial structure helps the model learn more discriminative features.\n",
    "\n",
    "**Looking at the hardest examples**: Even the CNN's mistakes are often on genuinely difficult cases. Compare these to the linear model's errors - do you notice any patterns?\n",
    "\n",
    "**Key insight**: The CNN doesn't just have more parameters—it has *better* parameters that match the structure of the problem.\n",
    "\n",
    "---\n",
    "\n",
    "**Comparison so far**:\n",
    "- Linear model: ~83% (treats image as flat vector)\n",
    "- CNN: ~91% (exploits spatial structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4239ef",
   "metadata": {},
   "source": [
    "## Part 4: Transfer to CIFAR-10\n",
    "\n",
    "Now let's test our CNN architecture on a more challenging dataset: **CIFAR-10**. These are 32×32 color images of real-world objects (planes, cars, animals, etc.).\n",
    "\n",
    "**Key differences from FashionMNIST**:\n",
    "- Color images (3 channels instead of 1)\n",
    "- More complex, natural images\n",
    "- More variation within each class\n",
    "\n",
    "Let's see how our CNN architecture performs without any modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd5e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "cifar_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])\n",
    "\n",
    "# Load and split CIFAR-10\n",
    "cifar_full_train = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=cifar_transform)\n",
    "\n",
    "# Split into train (40k) and validation (10k) - 80/20 split\n",
    "cifar_train_size = 40000\n",
    "cifar_val_size = len(cifar_full_train) - cifar_train_size\n",
    "cifar_train_ds, cifar_val_ds = random_split(cifar_full_train, [cifar_train_size, cifar_val_size],\n",
    "                                             generator=torch.Generator().manual_seed(67))\n",
    "\n",
    "cifar_test_ds = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=cifar_transform)\n",
    "\n",
    "# Batch size for CIFAR-10: Using 128 instead of 64\n",
    "# Larger batch = better GPU utilization for color images (3 channels vs 1)\n",
    "# Also provides more stable gradient estimates for this harder task\n",
    "cifar_batch = 128\n",
    "\n",
    "cifar_train_loader = DataLoader(cifar_train_ds, batch_size=cifar_batch, shuffle=True, num_workers=num_workers)\n",
    "cifar_val_loader = DataLoader(cifar_val_ds, batch_size=cifar_batch, shuffle=False, num_workers=num_workers)\n",
    "cifar_test_loader = DataLoader(cifar_test_ds, batch_size=cifar_batch, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "cifar_class_names = ['plane','car','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "print(f'CIFAR-10 — Train: {len(cifar_train_ds)}, Val: {len(cifar_val_ds)}, Test: {len(cifar_test_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b2b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CIFAR-10 samples\n",
    "imgs_c, labs_c = next(iter(cifar_train_loader))\n",
    "plt.figure(figsize=(12, 3))\n",
    "for i in range(8):\n",
    "    plt.subplot(1, 8, i+1)\n",
    "    img = imgs_c[i].permute(1, 2, 0).numpy()\n",
    "    img = (img * [0.247, 0.243, 0.261]) + [0.4914, 0.4822, 0.4465]\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(cifar_class_names[labs_c[i]], fontsize=9)\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Sample CIFAR-10 Images', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3098870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN on CIFAR-10 (note: in_channels=3 for RGB)\n",
    "# Using 20 epochs: CIFAR-10 is harder and benefits from longer training\n",
    "# Watch the training curves to see if we need early stopping!\n",
    "print(\"Training CNN on CIFAR-10...\")\n",
    "cnn_cifar = SimpleCNN(in_channels=3, num_classes=10).to(device)\n",
    "cnn_cifar, cifar_history = train_model(cnn_cifar, cifar_train_loader, cifar_val_loader, epochs=10, lr=1e-3)\n",
    "cifar_acc = evaluate(cnn_cifar, cifar_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc17a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for CIFAR-10\n",
    "plot_training_curves(cifar_history, title=\"CNN Training History (CIFAR-10, No Augmentation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba6a3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for CNN on CIFAR-10 (no augmentation)\n",
    "plot_confusion_matrix(cnn_cifar, cifar_test_loader, cifar_class_names,\n",
    "                     title=\"CNN Confusion Matrix (CIFAR-10, No Augmentation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31079a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show hardest examples for CIFAR-10\n",
    "show_hardest_examples(cnn_cifar, cifar_test_loader, cifar_class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d5890d",
   "metadata": {},
   "source": [
    "### Reflection: Performance on CIFAR-10\n",
    "\n",
    "The CNN achieves around **74% accuracy** on CIFAR-10. That's much lower than on FashionMNIST!\n",
    "\n",
    "**Why the drop?**\n",
    "- CIFAR-10 images are more complex and varied\n",
    "- Objects can appear at different scales, angles, and positions\n",
    "- Background clutter and occlusion\n",
    "- Only 40,000 training images for a harder task\n",
    "\n",
    "**Looking at the training curves**: Notice the gap between train and validation loss growing over time? This is **overfitting**! The model is starting to memorize training examples rather than learning generalizable patterns. This suggests we could benefit from:\n",
    "1. Early stopping (stop training when val loss stops improving)\n",
    "2. Regularization techniques\n",
    "3. **Data augmentation** (our next topic!)\n",
    "\n",
    "**Looking at the confusion matrix**, you'll notice interesting patterns:\n",
    "- **Cat vs. Dog**: A classic challenge even for humans with low-resolution images!\n",
    "- **Bird vs. Plane**: Both can appear in the sky with similar backgrounds\n",
    "- **Automobile vs. Truck**: Vehicles are often confused with each other\n",
    "\n",
    "**Looking at the hardest examples**: These reveal the challenges of CIFAR-10 - low resolution, occlusion, unusual angles, etc.\n",
    "\n",
    "**Question**: What modeling choice could help us make better use of our limited training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634be2e1",
   "metadata": {},
   "source": [
    "## Part 5: Data Augmentation as a Strategic Choice\n",
    "\n",
    "**The problem**: Our model is overfitting to specific details in the training images (exact positions, orientations, etc.).\n",
    "\n",
    "**The solution**: Use **data augmentation** to artificially expand our training set by applying random transformations that preserve the label. This induces *learnable invariance* into our dataset, that is, our dataset now *exemplifies* that image labels are invariant to image transformations like horizontal flips and cropping out some of the edge.\n",
    "\n",
    "For CIFAR-10, we'll use:\n",
    "- Random horizontal flips (a car is still a car when flipped)\n",
    "- Random crops (objects can appear anywhere in the frame)\n",
    "\n",
    "This is another *modeling choice*: we're encoding our knowledge that these transformations don't change the object's identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e1f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create augmented CIFAR-10 training set\n",
    "# Note: We only augment TRAINING data, never validation or test data!\n",
    "cifar_aug_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # 50% chance of horizontal flip\n",
    "    transforms.RandomCrop(32, padding=4),  # Random crop with padding\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])\n",
    "\n",
    "# Create new training set with augmentation\n",
    "cifar_train_aug_full = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=False, transform=cifar_aug_transform)\n",
    "cifar_train_aug_ds, _ = random_split(cifar_train_aug_full, [cifar_train_size, cifar_val_size],\n",
    "                                     generator=torch.Generator().manual_seed(67))\n",
    "\n",
    "cifar_train_aug_loader = DataLoader(cifar_train_aug_ds, batch_size=cifar_batch, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# Train CNN with augmentation\n",
    "print(\"Training CNN on CIFAR-10 (with augmentation)...\")\n",
    "cnn_cifar_aug = SimpleCNN(in_channels=3, num_classes=10).to(device)\n",
    "# We can train longer here without overfitting\n",
    "cnn_cifar_aug, cifar_aug_history = train_model(cnn_cifar_aug, cifar_train_aug_loader, cifar_val_loader, epochs=20, lr=1e-3)\n",
    "cifar_acc_aug = evaluate(cnn_cifar_aug, cifar_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c326e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves with augmentation\n",
    "plot_training_curves(cifar_aug_history, title=\"CNN Training History (CIFAR-10, With Augmentation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d0b0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for CNN on CIFAR-10 (with augmentation)\n",
    "plot_confusion_matrix(cnn_cifar_aug, cifar_test_loader, cifar_class_names,\n",
    "                     title=\"CNN Confusion Matrix (CIFAR-10, With Augmentation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2123557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show hardest examples with augmentation\n",
    "show_hardest_examples(cnn_cifar_aug, cifar_test_loader, cifar_class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db173bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions with augmented model\n",
    "cnn_cifar_aug.eval()\n",
    "imgs_c, labs_c = next(iter(cifar_test_loader))\n",
    "imgs_c, labs_c = imgs_c.to(device), labs_c.to(device)\n",
    "with torch.no_grad():\n",
    "    outs_c = cnn_cifar_aug(imgs_c[:8])\n",
    "    preds_c = outs_c.argmax(dim=1)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "for i in range(8):\n",
    "    plt.subplot(1, 8, i+1)\n",
    "    img = imgs_c[i].cpu().permute(1, 2, 0).numpy()\n",
    "    img = (img * [0.247, 0.243, 0.261]) + [0.4914, 0.4822, 0.4465]\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    correct = '✓' if preds_c[i] == labs_c[i] else '✗'\n",
    "    plt.title(f'{correct} {cifar_class_names[preds_c[i]]}', fontsize=9,\n",
    "              color='green' if preds_c[i] == labs_c[i] else 'red')\n",
    "    plt.axis('off')\n",
    "plt.suptitle('CNN Predictions on CIFAR-10 (with augmentation)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77d12a3",
   "metadata": {},
   "source": [
    "### Reflection: The Impact of Augmentation\n",
    "\n",
    "With augmentation, accuracy improves to around **77%**. While this might seem marginal, it's worth noting that data augmentation also improves the ability for the model to *generalize* to new examples outside the distribution of the dataset as well as preventing overfitting (we can train for longer without the model memorizing data).\n",
    "\n",
    "**Looking at the training curves**: Compare the two CIFAR-10 training curves (scroll up). With augmentation:\n",
    "- The gap between train and val loss is **smaller** - less overfitting!\n",
    "- Training loss is higher (model can't perfectly memorize augmented data)\n",
    "- Validation loss is lower (better generalization)\n",
    "- We could potentially train even longer without overfitting\n",
    "\n",
    "**Why does augmentation help more on CIFAR-10 than it would on FashionMNIST?**\n",
    "- CIFAR-10 has more variation in object position, scale, and orientation\n",
    "- The training set is relatively small for the complexity of the task\n",
    "- Augmentation effectively teaches the model to be invariant to these transformations\n",
    "\n",
    "**Key insight**: Data augmentation is most valuable when:\n",
    "1. The task has natural invariances (flips, rotations, crops)\n",
    "2. The test-time data is expected to vary along these transformations\n",
    "3. The task is complex and the dataset is not expected to exemplify these invariances without augmentation\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: Try adding augmentation to FashionMNIST (you'd see minimal improvement) vs. CIFAR-10 (clear improvement). This illustrates how modeling choices interact with dataset characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928bcecd",
   "metadata": {},
   "source": [
    "## Summary: Modeling Choices Matter\n",
    "\n",
    "Let's review what we've learned about how different choices affect performance:\n",
    "\n",
    "| Model | Dataset | Accuracy | Key Insight |\n",
    "|-------|---------|----------|-------------|\n",
    "| Linear | FashionMNIST | ~84% | Surprisingly effective on simple, aligned images |\n",
    "| CNN | FashionMNIST | ~92% | Spatial inductive bias helps significantly |\n",
    "| CNN | CIFAR-10 (no aug) | ~74% | Same architecture, harder dataset, shows overfitting |\n",
    "| CNN | CIFAR-10 (aug) | ~77% | Augmentation reduces overfitting and improves generalization |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Inductive bias matters**: The CNN's spatial structure assumption led to a 6% improvement on FashionMNIST\n",
    "2. **Dataset characteristics matter**: The same CNN performs differently on FashionMNIST vs. CIFAR-10\n",
    "3. **Strategic augmentation matters**: Augmentation provides a bigger boost on complex, varied datasets\n",
    "4. **Overfitting detection is crucial**: Training curves reveal when models memorize vs. generalize\n",
    "5. **Machine learning is modeling**: Every choice (architecture, data representation, augmentation) encodes assumptions about the problem\n",
    "\n",
    "### Questions for Further Exploration\n",
    "\n",
    "- Why didn't we use augmentation on FashionMNIST? (Try it and see!)\n",
    "- What other augmentations might help on CIFAR-10? (color jittering, cutout, etc.)\n",
    "- How would a linear model perform on CIFAR-10? (Spoiler: poorly!)\n",
    "- What if we trained for more epochs? Used a deeper network?\n",
    "\n",
    "The point isn't to achieve state-of-the-art results—it's to understand how and why different modeling choices lead to different outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd9c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Print final comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Linear Model (FashionMNIST):          {linear_acc:.2f}%\")\n",
    "print(f\"CNN (FashionMNIST):                   {cnn_acc:.2f}%\")\n",
    "print(f\"CNN (CIFAR-10, no augmentation):      {cifar_acc:.2f}%\")\n",
    "print(f\"CNN (CIFAR-10, with augmentation):    {cifar_acc_aug:.2f}%\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- CNN improves over Linear by ~8% on FashionMNIST\")\n",
    "print(\"- CIFAR-10 is much harder (~17% drop from FashionMNIST)\")\n",
    "print(\"- Augmentation helps reduce overfitting on CIFAR-10\")\n",
    "print(\"- Check training curves to understand generalization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7544c43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introduction-to-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
